{"pageProps":{"note":{"id":"1rm9i6wwcy7k2rkioy52rs3","title":"Language Autoencoding","desc":"","updated":1658949026452,"created":1658941369199,"custom":{},"fname":"ref.nlp.lae","type":"note","vault":{"fsPath":"vault"},"contentHash":"e0f9b93b0699d46bbc37ab24580a4506","links":[{"from":{"fname":"ref.arch.s4","vaultName":"vault"},"type":"backlink","position":{"start":{"line":21,"column":10,"offset":3571},"end":{"line":21,"column":25,"offset":3586},"indent":[]},"value":"ref.nlp.lae","alias":"ref.nlp.lae"}],"anchors":{},"children":[],"parent":"jlbuz8giyj93jsc1l0lmbzs","data":{}},"body":"<h1 id=\"language-autoencoding\"><a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#language-autoencoding\"></a>Language Autoencoding</h1>\n<p><strong>L</strong>anguage <strong>A</strong>uto<strong>E</strong>ncoding, or LAE, is a transformer objective proposed by <a href=\"https://arxiv.org/abs/2004.08097\">Shin, et al., 2020</a> for training self-attention mechanisms for bidirectional NLU by explicitly masking out dot-product self-attention matrices with large negative bias on the diagonal, essentially \"disabling\" the model's internal representation that tokens \"attend\" to themselves and requiring each token to be predicted from surrounding context. </p>\n<p><img src=\"/vault/assets/images/lae.png\" alt=\"Language Autoencoding, from Shin, et al.\"></p>\n<p>Shin, et al. acknowledge the relatedness of their method to masked language modeling, or MLM, objectives such as the one used to fit <a href=\"https://arxiv.org/abs/1810.04805\">BERT</a> (Devlin et al., 2018), drawing the analogical relationship of autoencoders to denoising autoencoders. I suspect LAE may be a more sample-efficient and smoother objective than BERT, while not requiring the same computational overhead and mind-bending technical considerations as <a href=\"https://arxiv.org/abs/1906.08237\">XLNet</a>, as it requires learning bidirectional representations between all tokens and introduces no incongruities between train- and test-time as BERT does with its <code>[MASK]</code> token.</p>\n<p>Self-attention masks could also potentially be used for regularization along alternative methods that explicitly perturb input sequences such as <a href=\"https://arxiv.org/abs/1906.08101\">whole-word masking</a> by being more selective about which positions are \"masked\" on the diagonal.</p>\n<p><img src=\"/vault/assets/images/diag_mask.png\" alt=\"Diagonal Masking, from Shin, et al.\"></p>\n<hr>\n<strong>Backlinks</strong>\n<ul>\n<li><a href=\"/vault/notes/gj4j8mv891v6ofxgbv7a3fu\">Structured State Space Sequence Models (vault)</a></li>\n</ul>","noteIndex":{"id":"ihfr21mlvz5lp3n62uu344t","title":"kavlogs","desc":"","updated":1659115143090,"created":1659018913792,"custom":{"nav_order":0,"permalink":"/"},"fname":"ref","type":"note","vault":{"fsPath":"vault"},"contentHash":"41b7e8cfe841b035025dfa85ec0b142e","links":[],"anchors":{},"children":["39acu8nb3xn6t18xul0ji8d","jc56h8njgqawadlroex4ju1","wf5gku64lj2ag6n8kzs9o8f","jlbuz8giyj93jsc1l0lmbzs","3c09ct2zvlk9dfitxexbss3"],"parent":null,"data":{},"body":"This is an assorted collection of notes and thoughts on things I've read about before. Here for my reference, and so that you can come here to read my background instead of receiving an untimely info-dump whose content just amounts to a recitation of anything here, out of mutual respect for my time and yours."},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true,"enableSelfContainedVaults":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":true,"vaultSelectionModeOnCreate":"smart","leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2}},"randomNote":{},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"copyNoteLink":{},"templateHierarchy":"template"},"workspace":{"vaults":[{"fsPath":"vault"},{"fsPath":"projects"},{"fsPath":"tenets"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"task":{"name":"task","dateFormat":"y.MM.dd","addBehavior":"asOwnDomain","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"taskCompleteStatus":["done","x"],"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableHandlebarTemplates":true,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"enableUserTags":true,"enableHashTags":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":false,"enableEditorDecorations":true,"maxPreviewsCached":10,"maxNoteLength":204800,"enableFullHierarchyNoteTitle":false,"enableSmartRefs":true,"dendronVersion":"0.105.1"},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":true},"publishing":{"theme":"dark","enableFMTitle":true,"enableNoteTitleForLink":true,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteUrl":"https://kavorite.github.io","assetsPrefix":"/vault","siteHierarchies":["ref","pub"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"âš‘","description":"kavlogs"},"github":{"enableEditLink":false,"editBranch":"main","editViewMode":"tree","editLinkText":"Edit this page on GitHub"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"siteFaviconPath":"favicon.ico","siteIndex":"ref"}}},"__N_SSG":true}
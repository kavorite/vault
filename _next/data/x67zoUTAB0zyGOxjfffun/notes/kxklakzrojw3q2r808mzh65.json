{"pageProps":{"note":{"id":"kxklakzrojw3q2r808mzh65","title":"Continuous Kernel Convolution","desc":"","updated":1659020720817,"created":1658835909947,"custom":{},"fname":"ref.arch.ckconv","type":"note","vault":{"fsPath":"vault"},"contentHash":"b8c09903ec43950ae72cafb19cf17e26","links":[{"type":"wiki","from":{"fname":"ref.arch.ckconv","id":"kxklakzrojw3q2r808mzh65","vaultName":"vault"},"value":"ref.opt.film","alias":"ref.opt.film","position":{"start":{"line":13,"column":491,"offset":2112},"end":{"line":13,"column":507,"offset":2128},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"ref.opt.film"}},{"type":"wiki","from":{"fname":"ref.arch.ckconv","id":"kxklakzrojw3q2r808mzh65","vaultName":"vault"},"value":"ref.opt.loss-conditioning","alias":"ref.opt.loss-conditioning","position":{"start":{"line":13,"column":649,"offset":2270},"end":{"line":13,"column":678,"offset":2299},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"ref.opt.loss-conditioning"}},{"from":{"fname":"ref.cv.stego","vaultName":"vault"},"type":"backlink","position":{"start":{"line":3,"column":211,"offset":570},"end":{"line":3,"column":230,"offset":589},"indent":[]},"value":"ref.arch.ckconv","alias":"ref.arch.ckconv"},{"from":{"fname":"ref.arch.deq.extensions","vaultName":"vault"},"type":"backlink","position":{"start":{"line":14,"column":164,"offset":2431},"end":{"line":14,"column":192,"offset":2459},"indent":[]},"value":"ref.arch.ckconv","alias":"FlexConv"}],"anchors":{"computational-caveats":{"type":"header","text":"Computational Caveats","value":"computational-caveats","line":14,"column":0,"depth":2}},"children":[],"parent":"39acu8nb3xn6t18xul0ji8d","data":{}},"body":"<h1 id=\"continuous-kernel-convolution\"><a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#continuous-kernel-convolution\"></a>Continuous Kernel Convolution</h1>\n<p>Continuous-kernel convolutions are just that: Convolutions with a continuous kernel. The kernel is determined by a <strong>generator network</strong> with a fixed parameter cost, by composing a learned linear operator with a predetermined gaussian mask, such as a blur filter, or an (optionally steerable) Gabor filter as in <a href=\"https://arxiv.org/abs/2110.08059\">Romero, et al., 2021's</a> <strong>M</strong>ultiplicative <strong>A</strong>nisotropic <strong>G</strong>abor <strong>N</strong>etworks, or MAGNets.</p>\n<p><img src=\"/vault/assets/images/flexconv.png\" alt=\"FlexConv parametrization\"></p>\n<p>In order to sample a kernel, a small, feed-forward subnetwork is used to map relative spatial coordinates sampled at some discrete rate— e.g., a grid on [-1, 1]— to intensities within the desired output space. The output of the MLP can be stabilized with methods such as LayerNorm, weight standardization etc., but the important bit is that the MLP learns a continuous function that maps points in coordinate space to some learned intensity for the output kernel at the given point, rather than fixing the kernel's size and depth and learning the values of each constituent point within the kernel's volumetric mapping individually. </p>\n<h2 id=\"computational-caveats\"><a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#computational-caveats\"></a>Computational Caveats</h2>\n<p>Romero, et al. observed based on the states of converged FlexConvs that they tended to learn larger receptive fields as one descends deeper into their architecture during training, at least on natural images. This presented a problem: It slowed things down. The computational overhead of convolutions scales cubically with kernel size, and the only hard upper bound is the input sampling resolution.</p>\n<p><img src=\"/vault/assets/images/progressive-kernel-sizes.png\" alt=\"Progressively Larger Kernel Sizes\"></p>\n<p>This cost could of course be offset with downsampling, but <a href=\"https://arxiv.org/abs/2206.03398\">CCNN</a> offers evidence that depthwise-separable versions of <a href=\"https://arxiv.org/abs/2110.08059\">FlexConv</a> kernels, or FlexSepConv, can be combined with global kernel sizes or depthwise-separable convolutions in order to offset the computational cost of large kernels, making it quadratic instead. The size of the receptive field produced by kernel generator networks like MAGNets could additionally be conditioned on its inputs with intermediate componentry akin to <a href=\"/vault/notes/zy4oqk1z59whxqbjtmwbdm4\">Feature-wise Linear Modulation</a>, or explicitly regularized with an additional loss term, leading me to ask what the result would be if these architectures were trained with <a href=\"/vault/notes/0oj3xw4fhwwb7qifcve8g19\">Loss Conditioning</a>.</p>\n<hr>\n<strong>Backlinks</strong>\n<ul>\n<li><a href=\"/vault/notes/hqv5s3hlhjqatutciw4x4nm\">STEGO (vault)</a></li>\n<li><a href=\"/vault/notes/txmy1anxf7bhr607yyk7te7\">DEQ Extensions (vault)</a></li>\n</ul>","noteIndex":{"id":"ihfr21mlvz5lp3n62uu344t","title":"kavlogs","desc":"","updated":1659115143090,"created":1659018913792,"custom":{"nav_order":0,"permalink":"/"},"fname":"ref","type":"note","vault":{"fsPath":"vault"},"contentHash":"41b7e8cfe841b035025dfa85ec0b142e","links":[],"anchors":{},"children":["39acu8nb3xn6t18xul0ji8d","jc56h8njgqawadlroex4ju1","wf5gku64lj2ag6n8kzs9o8f","jlbuz8giyj93jsc1l0lmbzs","3c09ct2zvlk9dfitxexbss3"],"parent":null,"data":{},"body":"This is an assorted collection of notes and thoughts on things I've read about before. Here for my reference, and so that you can come here to read my background instead of receiving an untimely info-dump whose content just amounts to a recitation of anything here, out of mutual respect for my time and yours."},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true,"enableSelfContainedVaults":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":true,"vaultSelectionModeOnCreate":"smart","leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2}},"randomNote":{},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"copyNoteLink":{},"templateHierarchy":"template"},"workspace":{"vaults":[{"fsPath":"vault"},{"fsPath":"projects"},{"fsPath":"tenets"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"task":{"name":"task","dateFormat":"y.MM.dd","addBehavior":"asOwnDomain","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"taskCompleteStatus":["done","x"],"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableHandlebarTemplates":true,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"enableUserTags":true,"enableHashTags":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":false,"enableEditorDecorations":true,"maxPreviewsCached":10,"maxNoteLength":204800,"enableFullHierarchyNoteTitle":false,"enableSmartRefs":true,"dendronVersion":"0.105.1"},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":true},"publishing":{"theme":"dark","enableFMTitle":true,"enableNoteTitleForLink":true,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteUrl":"https://kavorite.github.io","assetsPrefix":"/vault","siteHierarchies":["ref","pub"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"⚑","description":"kavlogs"},"github":{"enableEditLink":false,"editBranch":"main","editViewMode":"tree","editLinkText":"Edit this page on GitHub"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"siteFaviconPath":"favicon.ico","siteIndex":"ref"}}},"__N_SSG":true}
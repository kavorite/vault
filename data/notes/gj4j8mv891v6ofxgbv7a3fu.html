<h1 id="structured-state-space-sequence-models"><a aria-hidden="true" class="anchor-heading icon-link" href="#structured-state-space-sequence-models"></a>Structured State Space Sequence Models</h1>
<p><a href="https://arxiv.org/abs/2111.00396">S4</a> stands for "Efficiently Modeling Long Sequences with Structured State Spaces," an acronym coined in its publication of the same name by <a href="https://arxiv.org/abs/2111.00396">Gu et al., 2021.</a> This paper uses a continuous-time parametrization to model long-range dependencies in terms of multiple independent linear state space models in 1D sequences: But more importantly, the authors derive a practical optimization framework for such models that enables trivially data-parallel training and causal, autoregressive inference in <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord">1</span><span class="mclose">)</span></span></span></span></span> space by discretizing this continuous-time parametrization on-the-fly.</p>
<p>Causal convolutional kernels the same width as the input signal <em>at arbitrary timescales</em> are useful for fitting the model to signals causally during optimization, but alternatively, the underlying parameterization can be instead discretized as parameters for a <em>polynomial recurrence model</em> that explicitly stores the underlying "state" of the system between "transitions" (e.g., HiPPO kernels) between applications of the underlying discrete update step, or "state transitions—" allowing reconstruction of past context not only theoretically but <em>empirically.</em> This allows the model not only to scale log-linearly in the parameter count necessary to model the sequence length that it is expected to encounter. Deep SSMs provide a very promising alternative to transformer architectures, particularly for irregularly-sampled signals or extremely long contexts.</p>
<p><img src="/vault/assets/images/s4.png" alt="S4"></p>
<p>Offshoots include <a href="https://arxiv.org/abs/2206.11893">S4D</a>. Gu et al. found that "restricting the [state transition matrix] to be fully diagonal can still preserve the performance of the original model." Empirically, this works best when the continuous-time parametrization of S4 is left at the mercy of a discrete input signal which does not obey so many temporally-correlated signals as to be encapsulated with only one state space model: I.e., rather than the problem requiring reconstruction of continuous patterns, it merely requires reconstructing a specific subset of discrete signals. Slimming down the intermediate S4 module is thereby sometimes empirically observed to have better convergence properties and make better use of the same number of parameters on language tasks. <a href="https://arxiv.org/abs/2206.13947">Mehta, et al., 2022</a> found that a learnable parameter, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Δ</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\Delta,</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord">Δ</span><span class="mpunct">,</span></span></span></span></span> could even be completely fixed (set to 1) in their experiments without impacting performance— although they did acknowledge that they trained on relatively shorter sequence lengths, and used vastly more compute. </p>
<p><img src="/vault/assets/images/s4d.png" alt="S4D"></p>
<h3 id="arbitrary-temporal-masking"><a aria-hidden="true" class="anchor-heading icon-link" href="#arbitrary-temporal-masking"></a>Arbitrary temporal masking</h3>
<p>For self-attention models, it's trivial to prevent models from erroneously learning relationships between or within arbitrary points in a temporal sequence by adding a large bias to their intermediate attention logits, directly modifying the pointwise mutual information available to the model during training.</p>
<p><img src="/vault/assets/images/diag_mask.png" alt="Shin, et al., 2020."></p>
<p>Preventing attention <em>between</em> samples that occur in the same position within a minibatch using a method like <a href="https://arxiv.org/abs/2107.02027">SPFHP</a> (Kosec, et al., 2021) may be possible with S4 by concatenating convolutional kernels sampled for two different temporal signals. It isn't as obvious how to prevent the model from erroneously learning relationships between points <em>within</em> a sequence for which only one temporal kernel of uniform width can be computed, however, which may entail a need to compute multiple kernels of limited contextual length in order to perform arbitrary masking between points within a sequence (obviously not ideal).</p>
<p>Although <a href="/vault/notes/1rm9i6wwcy7k2rkioy52rs3">Language Autoencoding</a> was originally intended for attention models such as transformers and their derivatives— which enjoy great ease of implementation for this method because of their explicit representation of the pointwise mutual information between discrete points within a given sequence— In principle, it could work for arbitrary sequence models if there were some way to "block" arbitrary points within a given sequence from influencing one another on a forward pass in the same way. For something like S4, the solution does not immediately present itself, although the problem of preventing such models from erroneously finding relationships <em>between</em> samples within a forward pass seems less challenging, as one may simply choose to concatenate two convolutional kernels the same width as the respective temporal signals. </p>
<hr>
<strong>Backlinks</strong>
<ul>
<li><a href="/vault/notes/hwm8yedokqpoldsx5rsjghh">TIRR (vault)</a></li>
</ul>
<h1 id="unclip"><a aria-hidden="true" class="anchor-heading icon-link" href="#unclip"></a>UnCLIP</h1>
<p><a href="https://cdn.openai.com/papers/dall-e-2.pdf">Dall⋅E</a> performs image generation and editing conditioned on semantic natural language prompts embedded in the prior given by a multimodal <a href="/vault/notes/63z6zjmnp5tg1pj2rwcfb2f">Contrastive Language-Image Pretraining</a> encoder. Most of the publication just talks about jointly training a transformer textual encoder with an image encoder. As seems to be OpenAI's trademark, most of this discussion is making a lot of hay over very little actual novelty in its methods, and is mostly just impressive in its thorough enumeration of implementation details and sheer scale. </p>
<p>The real meat of the publication lies in the review of a number of different approaches from literature for methods of image synthesis (<a href="https://cdn.openai.com/papers/dall-e-2.pdf">paper</a> Section 2, "Method"), which reconstruct images by conditioning on both explicit textual prompts and latent multimodal embeddings. This process is not trained jointly with CLIP. Instead, the "text-image codebook" is learned ahead of time. <a href="https://openreview.net/pdf?id=qw8AKxfYbI">Classifier-free guidance</a> is implemented by randomly dropping CLIP embeddings during UnCLIP's training regime.</p>
<p>The best confluence of sample diversity, visual fidelity, and aesthetic ratings by human viewers was found to use an ensemble of <a href="https://arxiv.org/abs/2010.02502">DDIM</a> models which feed forward into one another with intermediate steps which upsample to a higher resolution (64×64, 256×256, 1024×1024), although it is unclear from the paper exactly how each stage is parametrized. This three-stage pipeline and the intermediate upsampling steps are collectively called UnCLIP. In this regime, each successive upsampled image is noised essentially by applying a transposed block filter, rather than by directly sampling from a Gaussian distribution. </p>
<p>Denoising the image via inpainting while reproducing the same semantics as the original content becomes a nontrivial, denoising-autoencoding objective for which diffusion models were found to be best suited overall. Perhaps a decoder could perform better by allowing the upsampling factor to be conditioned on the input using <a href="/vault/notes/zy4oqk1z59whxqbjtmwbdm4">Feature-wise Linear Modulation</a> and dynamically learned?</p>
<hr>
<strong>Backlinks</strong>
<ul>
<li><a href="/vault/notes/txmy1anxf7bhr607yyk7te7">DEQ Extensions (vault)</a></li>
</ul>
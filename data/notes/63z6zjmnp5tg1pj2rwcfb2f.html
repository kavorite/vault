<h1 id="contrastive-language-image-pretraining"><a aria-hidden="true" class="anchor-heading icon-link" href="#contrastive-language-image-pretraining"></a>Contrastive Language-Image Pretraining</h1>
<p>Contrastive Language-Image Pretraining, or <a href="https://openai.com/blog/clip/">CLIP</a>, is a method which trains a language encoder jointly with an image decoder in a collaborative regime, distilling domain knowledge into an image encoder by using the language encoder's representations of image captions as a supervisory signal in a training regime almost identical to a parallel version of <a href="/vault/notes/9cismp3jsqbqx1pyso6xdc4">Supervised Contrastive Learning</a>. Unlike SCL and many related contrastive methods, CLIP has the distinction of embedding both input encoder representations into the same latent space, making it useful in multimodal modeling and conditioning generative applications on more than one type of support, as in <a href="https://openai.com/blog/dall-e/">Dall-E</a>'s unCLIP.</p>
<hr>
<strong>Backlinks</strong>
<ul>
<li><a href="/vault/notes/s2dvzt8effhbjxgsflxlw75">UnCLIP (vault)</a></li>
</ul>
<h1 id="deq-extensions"><a aria-hidden="true" class="anchor-heading icon-link" href="#deq-extensions"></a>DEQ Extensions</h1>
<h2 id="jacobian-regularization"><a aria-hidden="true" class="anchor-heading icon-link" href="#jacobian-regularization"></a><a href="http://implicit-layers-tutorial.org/deep_equilibrium_models/">Jacobian Regularization</a></h2>
<p>DEQs turned out not to work especially well in practice, learning hard-to-solve Jacobian matrices, which caused training to lose stability and become less computationally tractable over time. Bai et al., the same research group which invented DEQs, later presented methods that allow this behavior to be explicitly regularized, bringing their computational costs to par with those of explicit networks.</p>
<h2 id="skipdeq"><a aria-hidden="true" class="anchor-heading icon-link" href="#skipdeq"></a><a href="https://arxiv.org/abs/2201.12240">SkipDEQ</a></h2>
<p>Introduced by Pal, et al., 2022, <a href="https://arxiv.org/abs/2201.12240">SkipDEQs</a> combine DEQs with an explicit correction layer which conditions them on their inputs and predicts initial hidden states, not unlike <a href="/vault/notes/zy4oqk1z59whxqbjtmwbdm4">Feature-wise Linear Modulation</a>, stabilizing and expediting training. It is <a href="https://github.com/SciML/DeepEquilibriumNetworks.jl">implemented</a> in Julia. Based on the difficulties that vanilla DEQs have with matching the computational efficiency of deep explicit models, all practical implementations of DEQs should probably look something like this.</p>
<h2 id="mdeq"><a aria-hidden="true" class="anchor-heading icon-link" href="#mdeq"></a><a href="https://arxiv.org/abs/2006.08656">MDEQ</a></h2>
<p>I'm somewhat unimpressed with MDEQ. I don't think that downsampling to multiple resolutions at the input with weight-tying is the best way to exploit relationships between different feature scales. Resampling the input just so that it can be passed through the same DEQ cell layer multiple times in parallel results in loss of some simplicity compared to baseline explicit methods such as vanilla ResNets, which can just use intermediate downsampling during forward propagation. </p>
<p>MDEQ's construction is motivated by an observation of poor generalization performance between multiple feature scales in discrete convolutions in an environment where applications of convolutions are dominated by tasks where scale invariance is highly desirable, leading <a href="https://arxiv.org/abs/1512.03385">He, et al., 2015</a> and many others to perform intermediate downsampling, introducing explicitly hierarchical patterns to how spatially correlated inputs are processed. Aggressive downsampling leads both to considerable improvements in both computational tractability and feature reuse: A success replicated many times over both in discriminative and generative image processing applications, including by UNets, proposed by <a href="https://arxiv.org/abs/1505.04597">Ronneberger, et al., 2015</a>, NVAE, by <a href="https://arxiv.org/abs/2007.03898">Vahdat and Kautz, 2020</a>, and <a href="/vault/notes/s2dvzt8effhbjxgsflxlw75">UnCLIP</a>, by Ramesh, et al., 2022.</p>
<p>On that basis, one might replace <a href="https://arxiv.org/abs/2006.08656">MDEQ</a>'s approach of backpropagating through multiple predefined feature resolutions simultaneously with something more akin to a <a href="/vault/notes/kxklakzrojw3q2r808mzh65">FlexConv</a>, instead, which takes the analogous approach of simply broadening the receptive field with increasing depth using continuous kernels that boast a fixed parameter cost, or even explicitly parametrize over the spatial downsampling factor at each step, while constraining resolution to lie above some fixed threshold, reducing the number of hyperparameters required for intermediate pooling while keeping most, if not all of the resulting representational capacity and computational dividends.</p>
<h4 id="sam"><a aria-hidden="true" class="anchor-heading icon-link" href="#sam"></a><a href="/vault/notes/3cublmnyywkz5grltuf1253">SAM</a></h4>
<p>Other forms of explicit regularization might also be of use in combination with explicit regularization of the Jacobian: I'm not sure SAM has ever been applied in tandem with implicit differentiation, but I'm also not sure why it wouldn't work in principle. As noted by Bai, et al., more conventional regularization techniques such as weight standardization and feature- and batch-wise normalization are critical for convergence of "vanilla" DEQs in practice, pointing to a relationship between the explicit "differentiability" of deep neural networks when taken as fixed-iteration equilibrium solvers, and the ease of explicitly solving for points of equilibrium in DEQs. I would trade compute off for reliability, hyperparameter-stability, and sample efficiency any day of the week, implicit backpropagation or not. Perhaps I should more deeply explore whether this is possible. </p>
<hr>
<strong>Backlinks</strong>
<ul>
<li><a href="/vault/notes/vl3h77sg0urrzngvpa94cp0">Deep Equilibrium Models (vault)</a></li>
</ul>
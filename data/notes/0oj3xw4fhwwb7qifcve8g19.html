<h1 id="loss-conditioning"><a aria-hidden="true" class="anchor-heading icon-link" href="#loss-conditioning"></a>Loss Conditioning</h1>
<p><a href="https://ai.googleblog.com/2020/04/optimizing-multiple-loss-functions-with.html">Loss-conditional training</a> is a method which conditions a neural network on a hyperparameter vector <strong>ɑ</strong> (where <strong>ɑ</strong> > <strong>0</strong> and Σɑ = 1). <a href="/vault/notes/zy4oqk1z59whxqbjtmwbdm4">Feature-wise Linear Modulation</a> is used to condition a neural network on desired tradeoffs between multiple objectives at test-time through input of an arbitrary ɑ, and has even been observed to recover performance and teach more about the underlying semantics of a task by distilling knowledge of two separate objectives into deeper neural networks simultaneously. Performance generally suffers with networks that are not as deep.</p>
<hr>
<strong>Backlinks</strong>
<ul>
<li><a href="/vault/notes/kxklakzrojw3q2r808mzh65">Continuous Kernel Convolution (vault)</a></li>
<li><a href="/vault/notes/zy4oqk1z59whxqbjtmwbdm4">Feature-wise Linear Modulation (vault)</a></li>
</ul>
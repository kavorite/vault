<h1 id="language-autoencoding"><a aria-hidden="true" class="anchor-heading icon-link" href="#language-autoencoding"></a>Language Autoencoding</h1>
<p><strong>L</strong>anguage <strong>A</strong>uto<strong>E</strong>ncoding, or LAE, is a transformer objective proposed by <a href="https://arxiv.org/abs/2004.08097">Shin, et al., 2020</a> for training self-attention mechanisms for bidirectional NLU by explicitly masking out dot-product self-attention matrices with large negative bias on the diagonal, essentially "disabling" the model's internal representation that tokens "attend" to themselves and requiring each token to be predicted from surrounding context. </p>
<p><img src="/vault/assets/images/lae.png" alt="Language Autoencoding, from Shin, et al."></p>
<p>Shin, et al. acknowledge the relatedness of their method to masked language modeling, or MLM, objectives such as the one used to fit <a href="https://arxiv.org/abs/1810.04805">BERT</a> (Devlin et al., 2018), drawing the analogical relationship of autoencoders to denoising autoencoders. I suspect LAE may be a more sample-efficient and smoother objective than BERT, while not requiring the same computational overhead and mind-bending technical considerations as <a href="https://arxiv.org/abs/1906.08237">XLNet</a>, as it requires learning bidirectional representations between all tokens and introduces no incongruities between train- and test-time as BERT does with its <code>[MASK]</code> token.</p>
<p>Self-attention masks could also potentially be used for regularization along alternative methods that explicitly perturb input sequences such as <a href="https://arxiv.org/abs/1906.08101">whole-word masking</a> by being more selective about which positions are "masked" on the diagonal.</p>
<p><img src="/vault/assets/images/diag_mask.png" alt="Diagonal Masking, from Shin, et al."></p>
<hr>
<strong>Backlinks</strong>
<ul>
<li><a href="/vault/notes/gj4j8mv891v6ofxgbv7a3fu">Structured State Space Sequence Models (vault)</a></li>
</ul>
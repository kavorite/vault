<h1 id="deep-equilibrium-models"><a aria-hidden="true" class="anchor-heading icon-link" href="#deep-equilibrium-models"></a>Deep Equilibrium Models</h1>
<p><strong>D</strong>eep <strong>Eq</strong>uilibrium Models, or <a href="http://implicit-layers-tutorial.org/deep_equilibrium_models/">DEQs</a>, were introduced in <a href="https://arxiv.org/abs/1909.01377">Bai et al., 2019</a>. They exploit the observation that conventional, or "explicit," deep neural networks in practice generally coerce their intermediate state to a fixed point of "equilibrium." The publication builds on the conception of neural networks proposed by <a href="https://arxiv.org/abs/1512.03385">He, et al., 2015</a> as a series of residual functions <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><msub><mtext mathvariant="bold">z</mtext><mi>i</mi></msub><mo separator="true">,</mo><msub><mtext mathvariant="bold">x</mtext><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mtext mathvariant="bold">z</mtext><mi>i</mi></msub><mo stretchy="false">)</mo><mo>+</mo><msub><mtext mathvariant="bold">x</mtext><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">f(\textbf{z}_i, \textbf{x}_i) = f(\textbf{z}_{i}) + \textbf{x}_{i-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord text"><span class="mord textbf">z</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord text"><span class="mord textbf">x</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord text"><span class="mord textbf">z</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6528em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord text"><span class="mord textbf">x</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span></span></span></span></span>. The authors characterize such residual networks as fixed-iteration solvers that push <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="bold">z</mtext><mn>0</mn></msub><mo>…</mo><msub><mtext mathvariant="bold">z</mtext><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">\textbf{z}_0 \ldots \textbf{z}_n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5944em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord text"><span class="mord textbf">z</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord text"><span class="mord textbf">z</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span> toward some 'point of equilibrium' <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mtext mathvariant="bold">z</mtext><mo>⋆</mo></msup></mrow><annotation encoding="application/x-tex">\textbf{z}^\star</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6887em;"></span><span class="mord"><span class="mord text"><span class="mord textbf">z</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6887em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">⋆</span></span></span></span></span></span></span></span></span></span></span></span> at their output layer. The DEQ authors motivate this generalization by the fact that intermediate layers of neural networks often learn redundant and repeated transformations (<a href="https://arxiv.org/abs/2110.15225">Parnami, et al., 2021</a>), which is somewhat surprising, undermining the "mixture of experts" theory commonly understood to justify the performance of DNNs.</p>
<p>Residual architectures proliferate throughout across a wide variety of tasks in literature. Their ubiquity cannot be overstated: After the watershed moment the first ResNets brought about in image recognition they were adapted to many other applications. Currently, the overall "residual" structure of backbones whose layers propagate intermediate information through a form of 'skip connection' includes models used in <a href="https://jalammar.github.io/illustrated-bert/">natural language processing</a>, <a href="https://arxiv.org/abs/1708.05123">recommender systems</a>, <a href="https://arxiv.org/abs/2106.01621">audio classification</a>, and many more: All can be said to trace a common lineage back to the tradition of the first convolutional ResNets. </p>
<p>Due to the proliferation of intermediate normalization and residuals in state of the art overparametrized models, reapplying the same "residual block" of a given neural network to different inputs over and over again is often stable, with predictable, bounded outputs. This is what makes modern topologies converge despite their depth. In the tradition of weight sharing and "reversible" methods such as those proposed by <a href="https://arxiv.org/abs/2102.07870">Sander, et al., 2021</a> and <a href="https://arxiv.org/abs/2101.00234">Reid, et al., 2021</a>, and echoing <a href="https://arxiv.org/abs/2004.12993">early exit</a> literature, Bai, et al. contend that <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mtext mathvariant="bold">z</mtext><mo>⋆</mo></msup></mrow><annotation encoding="application/x-tex">\textbf{z}^\star</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6887em;"></span><span class="mord"><span class="mord text"><span class="mord textbf">z</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6887em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">⋆</span></span></span></span></span></span></span></span></span></span></span></span> can be solved for explicitly with multiple forward passes of the same layer and associated weights, while backpropagation is done using implicit differentiation... for <em>any type of neural network.</em> Hence, the distinction between "explicit" and "implicit" methods. </p>
<p>In practice, while it is perhaps somewhat surprising that a single residual layer being reapplied multiple times can represent a neural network of arbitrary depth, the use of a fixed-scale residual block recovers the <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord">1</span><span class="mclose">)</span></span></span></span></span> memory footprint and performance of previous works based on "reversible" explicit differentiation, often converging to just as good a solution— although <a href="/vault/notes/txmy1anxf7bhr607yyk7te7">some adjustments</a> are necessary to keep training stable and efficient, such as explicit regularization of the Jacobian. </p>
<hr>
<strong>Children</strong>
<ol>
<li><a href="/vault/notes/txmy1anxf7bhr607yyk7te7">DEQ Extensions</a></li>
</ol>
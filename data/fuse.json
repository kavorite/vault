{"keys":[{"path":["title"],"id":"title","weight":1,"src":"title","getFn":null},{"path":["body"],"id":"body","weight":1,"src":"body","getFn":null}],"records":[{"i":0,"$":{"0":{"v":"This page has not yet sprouted","n":0.408},"1":{"v":"[Dendron](https://dendron.so/) (the tool used to generate this site) lets authors selective publish content. You will see this page whenever you click on a link to an unpublished page\n\n![](https://foundation-prod-assetspublic53c57cce-8cpvgjldwysl.s3-us-west-2.amazonaws.com/assets/images/not-sprouted.png)","n":0.189}}},{"i":1,"$":{"0":{"v":"soapbox","n":1},"1":{"v":"Stories that motivate my study and application of technology. ","n":0.333}}},{"i":2,"$":{"0":{"v":"kavlogs","n":1},"1":{"v":"This is an assorted collection of notes and thoughts on things I've read about before. Here for my reference, and so that you can come here to read my background instead of receiving an untimely info-dump whose content just amounts to a recitation of anything here, out of mutual respect for my time and yours.","n":0.135}}},{"i":3,"$":{"0":{"v":"Optimization","n":1},"1":{"v":"Methods related to the particularities of gradient descent: Objectives, optimization and backpropagation algorithms, and infeeding techniques.","n":0.25}}},{"i":4,"$":{"0":{"v":"Sharpness Aware Minimization","n":0.577},"1":{"v":"**S**harpness **A**ware **M**inimization, or SAM, is a regularization technique introduced by [Foret, et al., 2020][abstract] used to train models which have the same generalization ability under datasets that are up to an order of magnitude smaller. The algorithm is so simple I have [implemented it myself,][implementation] and I can describe it here:\n1. Requirements:\n    1. $V \\coloneqq$ the set of model parameters $\\textbf{w}_0, \\textbf{w}_1\\ldots \\textbf{w}_n$\n    2. $\\textbf{v} \\coloneqq$ global parameter vector (flatten/concat $\\textbf{w}_i \\in V$)\n    3. $\\rho \\coloneqq$ A hyperparameter. Authors usually set to 0.05.\n2. Until converged:\n    1. Take a small, $\\rho$-sized ascent step:\n        $$\n        \\textbf{w}_i\n        \\leftarrow\n        \\textbf{w}_i \n        + \\rho\n        \\frac\n            {\\nabla_{L}\\textbf{w}_i}\n            {||\\textbf{v}||_2}\\\n        \\forall\\ \\textbf{w}_i \\in V\n        $$\n    2. Take an actual *descent* step using an \"inner\" optimizer, like SGD. This is typically more involved, but also better-studied (see review by [Ruder, et al., 2016][optimizers]).\n\nTo say nothing of how a more stable training regime helps address the ongoing replicability crisis in deep learning, using SAM also means less training time. \n\nThis means despite SAM's requirement of twice as much compute per forward-backward pass, in effect taking one step back for every step forward, the technique's uncanny ability to converge to a superior and more stable performance within half as many steps suggests that the method's sample efficiency offers some potential to help this method cut some costs despite itself, especially in terms of data procurement. [LookSAM] can help offset those compute costs if you're pulling your hair out over it.\n\n![Left/Right: SGD/SA-SGD](/assets/images/sam.png)\n\nLeft/Right: Visualizations of loss landscape surrounding trained ResNet, contrasting SGD/SA-SGD, from Foret, et al. Wider, deeper minima result in better calibrated models with more generalizable decision boundaries.\n\n## Derivatives and Extensions\n### [LookSAM] \nLiu, et al., 2022 use a projective approximation to compute the ascent gradient every *k* steps instead of re-computing it every time a step is taken. Shameless self-plug, my [implementation] includes an interface to this functionality.\n\n### [ASAM]\n**A**daptive SAM, introduced by Kwon, et al., 2021, re-weights the ascent directions by a more complex formula for each parameter norm rather than using the global norm. My [implementation] also includes a variant of this.\n\n### [δ-SAM][dsam]\nSAM with dynamic reweighting by Zhou, et al., 2021— I'll be honest, I just stopped reading this one as soon as I saw it required doing three forward passes. \n\n[Abstract]: https://arxiv.org/abs/2010.01412\n[ASAM]: https://arxiv.org/abs/2102.11600\n[DSAM]: https://arxiv.org/abs/2112.08772\n[LookSAM]: https://arxiv.org/abs/2203.02714\n\n[implementation]: https://github.com/kavorite/sam\n[optimizers]: https://arxiv.org/abs/1609.04747\n","n":0.051}}},{"i":5,"$":{"0":{"v":"Loss Conditioning","n":0.707},"1":{"v":"[Loss-conditional training][blog] is a method which conditions a neural network on a hyperparameter vector **ɑ** (where **ɑ** > **0** and Σɑ = 1). [[ref.opt.film]] is used to condition a neural network on desired tradeoffs between multiple objectives at test-time through input of an arbitrary ɑ, and has even been observed to recover performance and teach more about the underlying semantics of a task by distilling knowledge of two separate objectives into deeper neural networks simultaneously. Performance generally suffers with networks that are not as deep.\n\n[blog]: https://ai.googleblog.com/2020/04/optimizing-multiple-loss-functions-with.html","n":0.108}}},{"i":6,"$":{"0":{"v":"Knowledge Distillation","n":0.707},"1":{"v":"A method first appearing in a [paper of the same name][abstract] (Hinton et al., 2015), in which a large \"teacher\" model's predictions are used as pseudo-annotations for a more compact \"student.\" Emphasis mine. The most common variant uses standard KL Divergence, but some research shows empirically that [logit cloning][cloning] using mean squared error (Kim, et al., 2021) may have better convergence properties. Emphasis mine.\n\n> A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to **compress the knowledge in an ensemble into a single model** which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.\n\n[abstract]: https://arxiv.org/abs/1503.02531\n[cloning]: https://arxiv.org/abs/2105.08919","n":0.063}}},{"i":7,"$":{"0":{"v":"Hyperbolic Geometry","n":0.707},"1":{"v":"Hyperbolic geometry is highly useful for capturing semantic relationships between items. Particularly for domains with latent hierarchies: This can include word embeddings, and indeed, all other manner of graph embeddings. The ubiquity of applications was somewhat surprising to be considering the relative rarity of applied examples.\n\nThe main limitation of projecting latent embeddings onto the Riemannian manifold is the need for specialized gradient descent methods such as [RSGD] ([tutorial]), which I suspect limits this method's accessibility by making it a little less \"batteries-included\" and sometimes incompatible with alternatives. Embedding latent representations with Hyperbolic metrics on a Poincaré ball can either be viewed as a blessing or a curse in practice, and its tradeoffs with Euclidean spaces are not always clear. But it's my opinion that most often, with data that fit exponential frequency distributions or adhere to explicit hierarchies as observed by [Nickel, et al., 2017][word-embeddings], the additional representational capacity when working on a fixed compute budget and within a fixed dimensionality is well worth the cost.\n\nIn Euclidean geometry, because of the expectation that points are normally distributed, they must be clustered extremely tightly in order to form distinct groups ([De Sa et al., 2018][tradeoffs]). This is rather unfortunate for methods such as [[ref.cl.supcon]], which rely explicitly on a small selection of samples having strong positive relationships, and everything else tending to be embedded very far away because of the ease with which clusters emerge on the Poincaré unit sphere as compared with the Euclidean unit sphere. [Some research][image-embeddings] suggests that this is not always practical. A small distance between hyperbolic embeddings on a unit Poincaré sphere captures not one, but two semantic properties: \n1. \"betweenness\" of an item with in a hierarchy, and\n2. the \"subtree\" to which the item belongs.\n\nFollowing is a graphic depicting the learning of Euclidean word embeddings.\n\n![Euclidean word embeddings](/assets/images/euclidean.webp)\n\nNotice how deviating from the origin quickly forces subspaces to the edges of the unit sphere. This pattern requires far fewer dimensions to capture using hyperbolic geometry to distance neighborhoods from one another on a hyperbolic sphere, because it allows distances between neighborhoods to be defined by their centrality _with respect to the origin._ Contrast below.\n\n![Hyperbolic word embeddings](/assets/images/hyperbolic.webp)\n\nThis means hyperbolic geometry allows for more efficient embedding of latent hierarchies, which is reflective of many long-tailed frequency distributions. Natural language corpora, for example, contain latent hierarchies, because \"chinchilla\" is a type of \"mammal.\" Controlling for this property is important in a variety of information retrieval applications, most notably [bag-of-words document vectorization][tfidf].\n\n![English word frequencies](/assets/images/word_frequency.png)\n\nCertain publications have gotten around this by using bespoke loss functions to explicitly model the hierarchy of labels (e.g., for [ImageNet pretraining][pretraining]), but that takes a lot of effort: and it only encapsulates the hierarchies that are imposed by engineers, which doesn't allow models to efficiently _learn_ latent hierarchies from supervisory signals that may contain many latent hierarchical relationships, yet explicitly offer explicit insight into very few by way of supervisory signals. Hyperbolic embeddings are not just a way to regularize a somewhat noisy solution space, but also a promising approach to allowing our optimizers more degrees of freedom in designing good solutions, taking the effort of imposing hierarchical domain models for symbolic data through explicit use of \"SynSets\" out of our own hands.\n\n[rsgd]: https://arxiv.org/abs/1806.03417\n[tutorial]: https://lars76.github.io/2020/07/23/rsgd-in-pytorch.html\n[tradeoffs]: https://arxiv.org/abs/1804.03329\n[pretraining]: https://arxiv.org/abs/2104.10972\n[word-embeddings]: https://arxiv.org/abs/1705.08039\n[image-embeddings]: https://arxiv.org/abs/1904.02239v2\n[tfidf]: https://tfidf.com\n","n":0.043}}},{"i":8,"$":{"0":{"v":"Feature-wise Linear Modulation","n":0.577},"1":{"v":"[FiLM], or **F**eature-wise **L**inear **M**odulation (Perez, et al., 2017) is a method of injecting intermediate support information for a task into a visual feature extraction pipeline. This is accomplished by introducing a generator network which \"modulates\" feature-wise normalization parameters according to some continuous function conditioned on arbitrary data. Continuous 'modulations' of input samples were used in the original publication to condition neural networks on task specific information, sideloading semantically relevant 'queries.'\n\n## Applications\n[FiLM] has proven useful in training DNNs to fit to [[multiple objective functions simultaneously|ref.opt.loss-conditioning]], such that the tradeoff between objectives can be adjusted at inference-time, and in [deep ensembling][filmens] (Turkoglu et al., 2022), where multiple sets of normalization parameters can be learned in order to emulate multiple instantiations of a given model while sharing the rest of the parameter set. A forward pass is required for each set of learned \"feature modulations,\" but the additional performance gains come at a predetermined, fixed-parameter cost. \n\n\n[film]: https://arxiv.org/abs/1709.0771\n[filmens]: https://arxiv.org/abs/2206.00050","n":0.08}}},{"i":9,"$":{"0":{"v":"Data Echoing","n":0.707},"1":{"v":"See [ArXiV] entry. Emphasis mine.\n> In the twilight of Moore's law, GPUs and other specialized hardware accelerators have dramatically sped up neural network training. However, earlier stages of the training pipeline, such as disk I/O and data preprocessing, do not run on accelerators. As accelerators continue to improve, these earlier stages will increasingly become the bottleneck. In this paper, we introduce \"data echoing,\" which reduces the total computation used by earlier pipeline stages and speeds up training whenever computation upstream from accelerators dominates the training time. Data echoing **reuses (or \"echoes\") intermediate outputs from earlier pipeline stages in order to reclaim idle capacity.** We investigate the behavior of different data echoing algorithms on various workloads, for various amounts of echoing, and for various batch sizes. We find that in all settings, at least one data echoing algorithm can match the baseline's predictive performance using less upstream computation. **We measured a factor of 3.25 decrease in wall-clock time for ResNet-50 on ImageNet when reading training data over a network.**\n[arxiv]: https://arxiv.org/abs/1907.05550","n":0.077}}},{"i":10,"$":{"0":{"v":"Natural Language Processing","n":0.577},"1":{"v":"Generation and understanding of text. Calling it \"natural language processing\" when the field defines it so narrowly is a bit of a misnomer. But it's also convention. Sorry about that.\n","n":0.183}}},{"i":11,"$":{"0":{"v":"MMASS","n":1},"1":{"v":"[Siddhant, et al., 2022][paper], explore the applications of adding **M**ore MASS to methods originally introduced by [Song, et al., 2019][mass] (emphasis mine, they don't have a fun backronym). On multilingual, semi-supervised NMT, state of the art generalization was reported across arbitrary language pairs by applying self-supervised denoising-autoencoder objectives to high-resource language datasets. Their complaint about the \"unscalability\" of data procurement for low-resource languages seems to be openly taking aim at [M2M100], introduced by Fan, et al., 2020.\n\n> Achieving universal translation between all human language pairs is the holy-grail of machine translation (MT) research. While recent progress in massively multilingual MT is one step closer to reaching this goal, it is becoming evident that extending a multilingual MT system simply by training on more parallel data is unscalable, since the availability of labeled data for low-resource and non-English-centric language pairs is forbiddingly limited. To this end, we present a pragmatic approach towards building a multilingual MT model that covers hundreds of languages, **using a mixture of supervised and self-supervised objectives, depending on the data availability for different language pairs.** We demonstrate that the synergy between these two training paradigms enables the model to produce high-quality translations in the **zero-resource setting,** even surpassing supervised translation quality for low- and mid-resource languages. We conduct a wide array of experiments to understand the effect of the degree of multilingual supervision, domain mismatches and amounts of parallel and monolingual data on the quality of our self-supervised multilingual models. To demonstrate the scalability of the approach, we train models with **over 200** languages and demonstrate high performance on zero-resource translation on several previously under-studied languages. We hope our findings will serve as a stepping stone towards enabling translation for the next thousand languages.\n\n[paper]: https://arxiv.org/abs/2201.03110\n[mass]: https://arxiv.org/abs/1905.02450\n[m2m100]: https://arxiv.org/abs/2010.11125","n":0.059}}},{"i":12,"$":{"0":{"v":"Language Autoencoding","n":0.707},"1":{"v":"\n**L**anguage **A**uto**E**ncoding, or LAE, is a transformer objective proposed by [Shin, et al., 2020][paper] for training self-attention mechanisms for bidirectional NLU by explicitly masking out dot-product self-attention matrices with large negative bias on the diagonal, essentially \"disabling\" the model's internal representation that tokens \"attend\" to themselves and requiring each token to be predicted from surrounding context. \n\n![Language Autoencoding, from Shin, et al.](/assets/images/lae.png)\n\nShin, et al. acknowledge the relatedness of their method to masked language modeling, or MLM, objectives such as the one used to fit [BERT] (Devlin et al., 2018), drawing the analogical relationship of autoencoders to denoising autoencoders. I suspect LAE may be a more sample-efficient and smoother objective than BERT, while not requiring the same computational overhead and mind-bending technical considerations as [XLNet], as it requires learning bidirectional representations between all tokens and introduces no incongruities between train- and test-time as BERT does with its `[MASK]` token.\n\nSelf-attention masks could also potentially be used for regularization along alternative methods that explicitly perturb input sequences such as [whole-word masking][masking] by being more selective about which positions are \"masked\" on the diagonal.\n\n![Diagonal Masking, from Shin, et al.](/assets/images/diag_mask.png)\n\n[paper]: https://arxiv.org/abs/2004.08097\n[bert]: https://arxiv.org/abs/1810.04805\n[masking]: https://arxiv.org/abs/1906.08101\n[xlnet]: https://arxiv.org/abs/1906.08237","n":0.073}}},{"i":13,"$":{"0":{"v":"Computer Vision","n":0.707}}},{"i":14,"$":{"0":{"v":"UnCLIP","n":1},"1":{"v":"\n[Dall⋅E][paper] performs image generation and editing conditioned on semantic natural language prompts embedded in the prior given by a multimodal [[ref.cl.clip]] encoder. Most of the publication just talks about jointly training a transformer textual encoder with an image encoder. As seems to be OpenAI's trademark, most of this discussion is making a lot of hay over very little actual novelty in its methods, and is mostly just impressive in its thorough enumeration of implementation details and sheer scale. \n\nThe real meat of the publication lies in the review of a number of different approaches from literature for methods of image synthesis ([paper] Section 2, \"Method\"), which reconstruct images by conditioning on both explicit textual prompts and latent multimodal embeddings. This process is not trained jointly with CLIP. Instead, the \"text-image codebook\" is learned ahead of time. [Classifier-free guidance][clfree] is implemented by randomly dropping CLIP embeddings during UnCLIP's training regime.\n\nThe best confluence of sample diversity, visual fidelity, and aesthetic ratings by human viewers was found to use an ensemble of [DDIM] models which feed forward into one another with intermediate steps which upsample to a higher resolution (64×64, 256×256, 1024×1024), although it is unclear from the paper exactly how each stage is parametrized. This three-stage pipeline and the intermediate upsampling steps are collectively called UnCLIP. In this regime, each successive upsampled image is noised essentially by applying a transposed block filter, rather than by directly sampling from a Gaussian distribution. \n\nDenoising the image via inpainting while reproducing the same semantics as the original content becomes a nontrivial, denoising-autoencoding objective for which diffusion models were found to be best suited overall. Perhaps a decoder could perform better by allowing the upsampling factor to be conditioned on the input using [[ref.opt.film]] and dynamically learned?\n\n[paper]: https://cdn.openai.com/papers/dall-e-2.pdf\n[ddim]: https://arxiv.org/abs/2010.02502\n[clfree]: https://openreview.net/pdf?id=qw8AKxfYbI","n":0.058}}},{"i":15,"$":{"0":{"v":"STEGO","n":1},"1":{"v":"Introduced in [Hamilton, et al., 2022][paper] ([blog]), **S**elf-supervised **T**ransformer with **E**nergy-based **G**raph **O**ptimization, or STEGO, can induce semantic segmentations by fitting a new head to a pretrained image recognizer, allowing it to be trained with tiny datasets on commodity hardware after a more costly classifier pretraining step. \n\nThe reference implementation may be somewhat dependent on a fixed sampling resolution within the image encoder, which could potentially limit the method to use in tandem with Vision Transformers or fixed-width [[ref.arch.ckconv]] á la FlexConv (Romero, et al., 2021). However, upsampling the intermediate feature maps of a vanilla CNN to match the spatial resolution of the input might yield similar (although somewhat pixellated) results. I have not conducted my own experiments, I just figure there's probably _some_ reason they put the T in STEGO other than for the sake of the sexy backronym... I hope. \n\n> Unsupervised semantic segmentation aims to **discover and localize semantically meaningful categories within image corpora without any form of annotation.** To solve this task, algorithms must produce features for every pixel that are both semantically meaningful and compact enough to form distinct clusters. Unlike previous works which achieve this with a single end-to-end framework, we propose to separate feature learning from cluster compactification. Empirically, we show that current unsupervised feature learning frameworks already generate dense features whose correlations are semantically consistent. This observation motivates us to design STEGO (&(**S**elf-supervised **T**ransformer with **E**nergy-based **G**raph **O**ptimization), a novel framework that distills unsupervised features into high-quality discrete semantic labels. At the core of STEGO is a novel contrastive loss function that encourages features to form compact clusters while preserving their relationships across the corpora. STEGO yields a significant improvement over the prior state of the art, on both the CocoStuff and Cityscapes semantic segmentation challenges.\n\n[blog]: https://wpthemeblog.com/mit-team-introduces-stego-an-algorithm-that-can-jointly-detect-and-segment-things-down-to-the-last-pixel-without-any-human/\n[paper]: https://arxiv.org/abs/2203.08414","n":0.058}}},{"i":16,"$":{"0":{"v":"Optical Character Recognition","n":0.577},"1":{"v":"\nThis is an arena that maps raster images containing text, to bounding boxes around that text, and optionally serializes it into a machine-readable textual encoding format such as ASCII. \n\n- [EasyOCR](https://github.com/JaidedAI/EasyOCR/)\n    is multilingual, a decent candidate for pseudo-annotating parallel corpora.\n- [PaddleOCR](https://github.com/PaddlePaddle/PaddleOCR)\n    > PaddleOCR aims to create multilingual, awesome, leading, and practical OCR tools that help users train better models and apply them into practice.\n- [Tesseract.js](https://tesseract.projectnaptha.com/)\n    uses a rule-based document-layout technique.\n    > Tesseract.js is a pure Javascript port of the popular Tesseract OCR engine.\n    > This library supports more than 100 languages, automatic text orientation and script detection, a simple interface for reading paragraph, word, and character bounding boxes. Tesseract.js can run either in a browser and on a server with NodeJS.\n\n","n":0.091}}},{"i":17,"$":{"0":{"v":"Fast Knowledge Distillation for CNNs","n":0.447},"1":{"v":"Using methods published by [Shen, et al., 2021,][paper] it's possible to perform [[ref.opt.knowledge-distillation]] more sample-efficiently, echoing [[ref.cl.moco#Fast-MoCo]]. The elevator pitch is that you create a pseudo-annotated dataset from many augmentations of the same image, alongside the output of the teacher's head, effectively creating many samples per forward pass of the teacher network, and allowing for a bespoke form of [[ref.opt.data-echoing]]: Images can be loaded once, resampled, and shipped over for preprocessing _on the hardware accelerator itself,_ effectively providing a built-in echoing factor of however many augmented views are taken per image. Since I generally already apply sample-specific augmentations on the device during data echoing, the main performance gains to be expected are those from applying the teacher network to cropped spatial regions of the input (jigsaw assembly, again, similar to [[ref.cl]]).\n1. Apply a set of deterministic, pseudo-random data augmentations (e.g., with [RandAugment]). For most training regimes, this also should include a (deterministic) random crop. Store the result for retrieval in some capacity.\n2. Apply the teacher model to augmented samples; store extracted features alongside the augmentation hyperparameters (bounding boxes for cropping, transformations applied). Persist the result.\n    - In JAX, and other augmentation implementations with stateless PRNG, this could be as simple as making a note of the transformation [seed][jax-rng] such that it can be reproduced on accelerators during training using a single copy of the anchor image, which is what prompts my comparison to data echoing.\n3. Train the student model to emulate the output of the teacher. Can be done with standard KL divergence objectives or MSE for [logit cloning][cloning] if you're feeling *exotic.*\n\n[jax-rng]: https://jax.readthedocs.io/en/latest/jax.random.html\n[paper]: https://arxiv.org/abs/2112.01528\n[randaugment]: https://github.com/4rtemi5/imax\n[cloning]: https://arxiv.org/abs/2105.08919","n":0.061}}},{"i":18,"$":{"0":{"v":"Contrastive Learning","n":0.707},"1":{"v":"\nThis section is generally dedicated to discussing methods for learning _by contrast._ For example, in standard supervised classification, we backpropagate against a loss function that evaluates the divergence between a discrete probability distribution: one matrix of **predictions** $P ∈ ℝ^{b × n}$, and one matrix of **targets** $T ∈ \\{0, 1\\}^{b × n}$. \n\nIt's often found that this can be less sample-efficient: after all, this supervisory signal embeds no explicit information about what in a minibatch is related to whatever else, it only forces exact and explicit replication of human judgments. Each sample only informs the model's predictions of _that sample_ (conditional independence). Surprisingly, this is error metric is generally noisier and harder to fit than the alternative metrics resulting from _using the same core idea of divergence between probability distributions,_ but first, applying a few simple transformations, such that the distributions reflect whether two items are \"similar\" or not, rather than annotations of where they lie in some arbitrary $\\{0,\\ 1\\}^n$ label space.\n\nFirst it can be more numerically stable to project $P$ to lie on the unit hypersphere by normalizing the feature-wise magnitudes of its sample embeddings:\n$$\nP_{i,j} ← \\frac{P_{i,j}}{||P_{:,j}||_2}\n$$\n\nSuch that $PP^T$ will contain the pairwise cosine similarities of all the entries in any given batch of label logits $P_{b × n}$— or more commonly, $P_{b×d}$, where $d$ is some arbitrary, latent dimensionality. Then, instead of directly classifying $P$ and $T$, one _instead_ evaluates $PP^T$ for its fit to $TT^T$ using some differentiable loss defined in $ℝ^{b×b}$, usually some variant of cross-entropy.\n\nThe result is a feature extractor whose latent representations project \"like\" and \"unlike\" items into spatially distinct regions on the unit hypersphere, or even potentially some non-Euclidean space, like [[the Riemannian manifold|ref.opt.hyperbolics]]. This can be used to perform few-shot classification by examining distances of new instances to a set of a few \"exemplars\" in embedding space, or, as in [[ref.cl.supcon]], simply fed forward into a linear classifier to use the stronger supervisory signal offered by pairwise instance discrimination for more robust generalization performance.\n\nThere are also variants, such as [[ref.cl.moco]], which simply treat augmentations of the same sample as 'positive' during instance discrimination rather than relying on explicit support from human annotations (an objective formulation called InfoNCE). ","n":0.052}}},{"i":19,"$":{"0":{"v":"TIRR","n":1},"1":{"v":"The **T**emporal **I**mage-based **R**eciprocal **R**ecommender is a mutual recommender system published by [Neve and McConville, 2021][paper]. It is built from the ground up for conditioning on individual preferences to recommend _people_ to _one another_ by conditioning on associated samples of their content, making it instantly useful for content aggregation and social media applications.\n\n<!-- \nTODO: my homework \n\nThere's a couple of key reasons that it outperforms baseline ECommerce recommendation systems, not the least of which is described in a rather pithy observation made in the paper: The job of the system is not merely to maximize $P(A|B)$ that user Alice likes user Bob, or $P(B|A)$, the converse. The _objective..._ is to maximize $P(A|B) ∧ P(B|A)$, which is strictly a far rarer event given two geometrically distributed random variables. Interestingly, this makes the instance discrimination objective used to pretrain TIRR's visual discriminator [[far more tractable|ref.cl.supcon]]. -->\n\nThere are many technical extensions and improvements that can be made. For example, it only uses photographs, not natural-language descriptions. The kernel tricks employed by LSTMs and GRUs in order to give them \"long term\" memory by emulating tracing patterns observed in mammalian cognition actually forget rather quickly, making integration of a method such as [[ref.arch.s4]]'s HiPPO potentially somewhat useful, both for condition predictions on longer historical contexts and use convolutions for training in parallel. \n\nThis ordinarily \"catastrophic\" forgetting could, however, be seen as favorable for a temporal model which must rapidly learn to condition new predictions on user preference within one session of use and adapt to changing preferences in real-time: causal autoregression in fixed memory at test-time may be the most important pragmatic consideration.\n\n![TIRR's architecture](/assets/images/tirr.png)\n[paper]: https://arxiv.org/abs/2108.11714","n":0.061}}},{"i":20,"$":{"0":{"v":"Supervised Contrastive Learning","n":0.577},"1":{"v":"\n[Khosla, et al., 2021][paper] ([blog]) verified that supervised instance discrimination is more sample-efficient than direct supervision and leads to more distinct class embeddings, even when a set of training labels is provided for input samples. The publication attributes this property to the high number of negative instance samples available for providing supervisory signal on _one another_ within a minibatch.\n\nAccording to the developers of the [official implementation][github], at small minibatch sizes, a [[momentum encoder|ref.cl.moco]] and dictionary are necessary to attain competitive performance. However, it should be possible to offset this technical overhead and hyperparameter sensitivity in a multi-device training regime by performing a rendezvous to synchronize replicated latent representations across minibatches with functionality such as `jax.lax.all_gather`, or by adding more positive examples with strong data augmentation, akin to a supervised form of [[Fast-MoCo|ref.cl.moco#Fast-MoCo]]. \n\nIt is unclear how the added positive examples would affect stability or performance— particularly with noisy labels or multiple labels assigned to training instances. To help correct for this, and because the supervisory signal may come from an underlying symbolic hierarchy with this method, it may be worth exploring projection of this method's latent embedding space onto a [[Poincaré ball|ref.opt.hyperbolics]].\n\n![Supervised Contrastive Learning](/assets/images/supcon.png)\n\n[blog]: https://ai.googleblog.com/2021/06/extending-contrastive-learning-to.html\n[paper]: https://paperswithcode.com/paper/supervised-contrastive-learning\n[github]: https://github.com/HobbitLong/SupContrast","n":0.071}}},{"i":21,"$":{"0":{"v":"Momentum Contrast","n":0.707},"1":{"v":"Introduced by He, et al., 2019, **Mo**mentum **Co**ntrast or MoCo, uses a set of \"slow\" and \"fast\" parameters. A \"slow\" momentum encoder, with a small update rate of ρ ≤ 0.01, is interpolated toward the latest parameter vector at each descent step, and to perform self-supervised instance discrimination, its embeddings are used as supervisory signals. Like [[ref.cl.supcon]], it required large training batches, which its authors would attempt to rectify in MoCov2.\n\n## Derivatives\n### [MoCov2]\nIntroduced by (Chen et al., 2020), the momentum encoder is used to enqueue old embeddings to gradually build a \"dictionary\" of negative samples, pushing future representations away from the past. An MLP projection head and stronger data augmentations were used. \n\n![MoCov2](/assets/images/mocov2.png) \n\n### [SimMoCo and SimCo][simco]\nZhang, et al., 2022 eschew MoCov2's addition of a \"negative dictionary\" in favor of a \"dual-temperature\" approach. Their publication also contains a helpful rundown of the wider self-supervised contrastive literature (Section 7, \"A Unified Perspective on SSL and Beyond\").\n\n### [Fast-MoCo]\nThis method, published by Ci et al., 2022, bears some resemblance to [[ref.cv.fast-distillation]] and FAIR's [DINO] ([blog][dino-blog]; Caron et al., 2021) with its self-supervised \"collage assembly\" objective, applying InfoNCE to multiple augmented views of a set of objects.\n\n--- \n[moco]: https://arxiv.org/abs/1911.05722\n[mocov2]: https://arxiv.org/abs/2003.04297\n[simco]: https://arxiv.org/abs/2203.17248\n[fast-moco]: https://arxiv.org/abs/2207.08220\n[dino]: https://arxiv.org/abs/2104.14294\n[dino-blog]: https://ai.facebook.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training/","n":0.071}}},{"i":22,"$":{"0":{"v":"Contrastive Language-Image Pretraining","n":0.577},"1":{"v":"Contrastive Language-Image Pretraining, or [CLIP], is a method which trains a language encoder jointly with an image decoder in a collaborative regime, distilling domain knowledge into an image encoder by using the language encoder's representations of image captions as a supervisory signal in a training regime almost identical to a parallel version of [[ref.cl.supcon]]. Unlike SCL and many related contrastive methods, CLIP has the distinction of embedding both input encoder representations into the same latent space, making it useful in multimodal modeling and conditioning generative applications on more than one type of support, as in [Dall-E]'s unCLIP.\n\n[clip]: https://openai.com/blog/clip/\n[dall-e]: https://openai.com/blog/dall-e/","n":0.101}}},{"i":23,"$":{"0":{"v":"Architecture","n":1},"1":{"v":"\nThis section is dedicated to novel deep modules and topologies I've seen being applied and invented recently. I may eventually add some more pedestrian stuff like discrete convolutions, various types of low-rank gated RNNs, etc., but for right now other resources should suffice to get you conceptually up to speed on all of those. This is here for my reference.\n","n":0.129}}},{"i":24,"$":{"0":{"v":"Structured State Space Sequence Models","n":0.447},"1":{"v":"[S4] stands for \"Efficiently Modeling Long Sequences with Structured State Spaces,\" an acronym coined in its publication of the same name by [Gu et al., 2021.][s4] This paper uses a continuous-time parametrization to model long-range dependencies in terms of multiple independent linear state space models in 1D sequences: But more importantly, the authors derive a practical optimization framework for such models that enables trivially data-parallel training and causal, autoregressive inference in $O(1)$ space by discretizing this continuous-time parametrization on-the-fly.\n\nCausal convolutional kernels the same width as the input signal *at arbitrary timescales* are useful for fitting the model to signals causally during optimization, but alternatively, the underlying parameterization can be instead discretized as parameters for a *polynomial recurrence model* that explicitly stores the underlying \"state\" of the system between \"transitions\" (e.g., HiPPO kernels) between applications of the underlying discrete update step, or \"state transitions—\" allowing reconstruction of past context not only theoretically but *empirically.* This allows the model not only to scale log-linearly in the parameter count necessary to model the sequence length that it is expected to encounter. Deep SSMs provide a very promising alternative to transformer architectures, particularly for irregularly-sampled signals or extremely long contexts.\n\n![S4](/assets/images/s4.png)\n\nOffshoots include [S4D]. Gu et al. found that \"restricting the [state transition matrix] to be fully diagonal can still preserve the performance of the original model.\" Empirically, this works best when the continuous-time parametrization of S4 is left at the mercy of a discrete input signal which does not obey so many temporally-correlated signals as to be encapsulated with only one state space model: I.e., rather than the problem requiring reconstruction of continuous patterns, it merely requires reconstructing a specific subset of discrete signals. Slimming down the intermediate S4 module is thereby sometimes empirically observed to have better convergence properties and make better use of the same number of parameters on language tasks. [Mehta, et al., 2022][gss] found that a learnable parameter, $\\Delta,$ could even be completely fixed (set to 1) in their experiments without impacting performance— although they did acknowledge that they trained on relatively shorter sequence lengths, and used vastly more compute. \n\n\n![S4D](/assets/images/s4d.png)\n\n\n### Arbitrary temporal masking\n\nFor self-attention models, it's trivial to prevent models from erroneously learning relationships between or within arbitrary points in a temporal sequence by adding a large bias to their intermediate attention logits, directly modifying the pointwise mutual information available to the model during training.\n\n![Shin, et al., 2020.](/assets/images/diag_mask.png)\n\nPreventing attention _between_ samples that occur in the same position within a minibatch using a method like [SPFHP] (Kosec, et al., 2021) may be possible with S4 by concatenating convolutional kernels sampled for two different temporal signals. It isn't as obvious how to prevent the model from erroneously learning relationships between points _within_ a sequence for which only one temporal kernel of uniform width can be computed, however, which may entail a need to compute multiple kernels of limited contextual length in order to perform arbitrary masking between points within a sequence (obviously not ideal).\n\nAlthough [[ref.nlp.lae]] was originally intended for attention models such as transformers and their derivatives— which enjoy great ease of implementation for this method because of their explicit representation of the pointwise mutual information between discrete points within a given sequence— In principle, it could work for arbitrary sequence models if there were some way to \"block\" arbitrary points within a given sequence from influencing one another on a forward pass in the same way. For something like S4, the solution does not immediately present itself, although the problem of preventing such models from erroneously finding relationships _between_ samples within a forward pass seems less challenging, as one may simply choose to concatenate two convolutional kernels the same width as the respective temporal signals. \n\n[s4]: https://arxiv.org/abs/2111.00396\n[s4d]: https://arxiv.org/abs/2206.11893\n[gss]: https://arxiv.org/abs/2206.13947\n[spfhp]: https://arxiv.org/abs/2107.02027\n","n":0.04}}},{"i":25,"$":{"0":{"v":"Deep Equilibrium Models","n":0.577},"1":{"v":"**D**eep **Eq**uilibrium Models, or [DEQs][tutorial], were introduced in [Bai et al., 2019][paper]. They exploit the observation that conventional, or \"explicit,\" deep neural networks in practice generally coerce their intermediate state to a fixed point of \"equilibrium.\" The publication builds on the conception of neural networks proposed by [He, et al., 2015][resnets] as a series of residual functions $f(\\textbf{z}_i, \\textbf{x}_i) = f(\\textbf{z}_{i}) + \\textbf{x}_{i-1}$. The authors characterize such residual networks as fixed-iteration solvers that push $\\textbf{z}_0 \\ldots \\textbf{z}_n$ toward some 'point of equilibrium' $\\textbf{z}^\\star$ at their output layer. The DEQ authors motivate this generalization by the fact that intermediate layers of neural networks often learn redundant and repeated transformations ([Parnami, et al., 2021][redundancy]), which is somewhat surprising, undermining the \"mixture of experts\" theory commonly understood to justify the performance of DNNs.\n\nResidual architectures proliferate throughout across a wide variety of tasks in literature. Their ubiquity cannot be overstated: After the watershed moment the first ResNets brought about in image recognition they were adapted to many other applications. Currently, the overall \"residual\" structure of backbones whose layers propagate intermediate information through a form of 'skip connection' includes models used in [natural language processing][berts], [recommender systems][cross], [audio classification][audio], and many more: All can be said to trace a common lineage back to the tradition of the first convolutional ResNets. \n\nDue to the proliferation of intermediate normalization and residuals in state of the art overparametrized models, reapplying the same \"residual block\" of a given neural network to different inputs over and over again is often stable, with predictable, bounded outputs. This is what makes modern topologies converge despite their depth. In the tradition of weight sharing and \"reversible\" methods such as those proposed by [Sander, et al., 2021][rhonets] and [Reid, et al., 2021][sharing], and echoing [early exit][exits] literature, Bai, et al. contend that $\\textbf{z}^\\star$ can be solved for explicitly with multiple forward passes of the same layer and associated weights, while backpropagation is done using implicit differentiation... for *any type of neural network.* Hence, the distinction between \"explicit\" and \"implicit\" methods. \n\nIn practice, while it is perhaps somewhat surprising that a single residual layer being reapplied multiple times can represent a neural network of arbitrary depth, the use of a fixed-scale residual block recovers the $O(1)$ memory footprint and performance of previous works based on \"reversible\" explicit differentiation, often converging to just as good a solution— although [[some adjustments|ref.arch.deq.extensions]] are necessary to keep training stable and efficient, such as explicit regularization of the Jacobian. \n\n\n\n[audio]: https://arxiv.org/abs/2106.01621\n[cross]: https://arxiv.org/abs/1708.05123\n[paper]: https://arxiv.org/abs/1909.01377\n[berts]: https://jalammar.github.io/illustrated-bert/\n[exits]: https://arxiv.org/abs/2004.12993\n[resnets]: https://arxiv.org/abs/1512.03385\n[rhonets]: https://arxiv.org/abs/2102.07870\n[sharing]: https://arxiv.org/abs/2101.00234\n[redundancy]: https://arxiv.org/abs/2110.15225\n[tutorial]: http://implicit-layers-tutorial.org/deep_equilibrium_models/\n","n":0.049}}},{"i":26,"$":{"0":{"v":"DEQ Extensions","n":0.707},"1":{"v":"\n## [Jacobian Regularization][jacreg]\nDEQs turned out not to work especially well in practice, learning hard-to-solve Jacobian matrices, which caused training to lose stability and become less computationally tractable over time. Bai et al., the same research group which invented DEQs, later presented methods that allow this behavior to be explicitly regularized, bringing their computational costs to par with those of explicit networks.\n\n## [SkipDEQ][fastdeq]\nIntroduced by Pal, et al., 2022, [SkipDEQs][fastdeq] combine DEQs with an explicit correction layer which conditions them on their inputs and predicts initial hidden states, not unlike [[ref.opt.film]], stabilizing and expediting training. It is [implemented][fastdeq-impl] in Julia. Based on the difficulties that vanilla DEQs have with matching the computational efficiency of deep explicit models, all practical implementations of DEQs should probably look something like this.\n\n## [MDEQ]\n\nI'm somewhat unimpressed with MDEQ. I don't think that downsampling to multiple resolutions at the input with weight-tying is the best way to exploit relationships between different feature scales. Resampling the input just so that it can be passed through the same DEQ cell layer multiple times in parallel results in loss of some simplicity compared to baseline explicit methods such as vanilla ResNets, which can just use intermediate downsampling during forward propagation. \n\nMDEQ's construction is motivated by an observation of poor generalization performance between multiple feature scales in discrete convolutions in an environment where applications of convolutions are dominated by tasks where scale invariance is highly desirable, leading [He, et al., 2015][resnets] and many others to perform intermediate downsampling, introducing explicitly hierarchical patterns to how spatially correlated inputs are processed. Aggressive downsampling leads both to considerable improvements in both computational tractability and feature reuse: A success replicated many times over both in discriminative and generative image processing applications, including by UNets, proposed by [Ronneberger, et al., 2015][unets], NVAE, by [Vahdat and Kautz, 2020][nvae], and [[ref.cv.unclip]], by Ramesh, et al., 2022.\n\nOn that basis, one might replace [MDEQ]'s approach of backpropagating through multiple predefined feature resolutions simultaneously with something more akin to a [[FlexConv|ref.arch.ckconv]], instead, which takes the analogous approach of simply broadening the receptive field with increasing depth using continuous kernels that boast a fixed parameter cost, or even explicitly parametrize over the spatial downsampling factor at each step, while constraining resolution to lie above some fixed threshold, reducing the number of hyperparameters required for intermediate pooling while keeping most, if not all of the resulting representational capacity and computational dividends.\n\n#### [[SAM|ref.opt.sam]] \nOther forms of explicit regularization might also be of use in combination with explicit regularization of the Jacobian: I'm not sure SAM has ever been applied in tandem with implicit differentiation, but I'm also not sure why it wouldn't work in principle. As noted by Bai, et al., more conventional regularization techniques such as weight standardization and feature- and batch-wise normalization are critical for convergence of \"vanilla\" DEQs in practice, pointing to a relationship between the explicit \"differentiability\" of deep neural networks when taken as fixed-iteration equilibrium solvers, and the ease of explicitly solving for points of equilibrium in DEQs. I would trade compute off for reliability, hyperparameter-stability, and sample efficiency any day of the week, implicit backpropagation or not. Perhaps I should more deeply explore whether this is possible. \n\n[mdeq]: https://arxiv.org/abs/2006.08656\n[jacreg]: http://implicit-layers-tutorial.org/deep_equilibrium_models/\n[fastdeq]: https://arxiv.org/abs/2201.12240\n[fastdeq-impl]: https://github.com/SciML/DeepEquilibriumNetworks.jl\n[resnets]: https://arxiv.org/abs/1512.03385\n[unets]: https://arxiv.org/abs/1505.04597\n[nvae]: https://arxiv.org/abs/2007.03898","n":0.043}}},{"i":27,"$":{"0":{"v":"Continuous Kernel Convolution","n":0.577},"1":{"v":"Continuous-kernel convolutions are just that: Convolutions with a continuous kernel. The kernel is determined by a **generator network** with a fixed parameter cost, by composing a learned linear operator with a predetermined gaussian mask, such as a blur filter, or an (optionally steerable) Gabor filter as in [Romero, et al., 2021's][flexconv] **M**ultiplicative **A**nisotropic **G**abor **N**etworks, or MAGNets.\n\n![FlexConv parametrization](/assets/images/flexconv.png)\n\nIn order to sample a kernel, a small, feed-forward subnetwork is used to map relative spatial coordinates sampled at some discrete rate— e.g., a grid on [-1, 1]— to intensities within the desired output space. The output of the MLP can be stabilized with methods such as LayerNorm, weight standardization etc., but the important bit is that the MLP learns a continuous function that maps points in coordinate space to some learned intensity for the output kernel at the given point, rather than fixing the kernel's size and depth and learning the values of each constituent point within the kernel's volumetric mapping individually. \n\n\n## Computational Caveats\nRomero, et al. observed based on the states of converged FlexConvs that they tended to learn larger receptive fields as one descends deeper into their architecture during training, at least on natural images. This presented a problem: It slowed things down. The computational overhead of convolutions scales cubically with kernel size, and the only hard upper bound is the input sampling resolution.\n\n![Progressively Larger Kernel Sizes](/assets/images/progressive-kernel-sizes.png)\n\nThis cost could of course be offset with downsampling, but [CCNN] offers evidence that depthwise-separable versions of [FlexConv] kernels, or FlexSepConv, can be combined with global kernel sizes or depthwise-separable convolutions in order to offset the computational cost of large kernels, making it quadratic instead. The size of the receptive field produced by kernel generator networks like MAGNets could additionally be conditioned on its inputs with intermediate componentry akin to [[ref.opt.film]], or explicitly regularized with an additional loss term, leading me to ask what the result would be if these architectures were trained with [[ref.opt.loss-conditioning]].\n\n\n[flexconv]: https://arxiv.org/abs/2110.08059\n[ccnn]: https://arxiv.org/abs/2206.03398","n":0.056}}}]}

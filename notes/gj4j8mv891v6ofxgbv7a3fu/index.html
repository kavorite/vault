<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><link rel="icon" href="/favicon.ico"/><title>Structured State Space Sequence Models</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="kavlogs"/><meta property="og:title" content="Structured State Space Sequence Models"/><meta property="og:description" content="kavlogs"/><meta property="og:url" content="https://kavorite.github.io/vault/notes/gj4j8mv891v6ofxgbv7a3fu/"/><meta property="og:type" content="article"/><meta property="article:published_time" content="7/26/2022"/><meta property="article:modified_time" content="8/2/2022"/><link rel="canonical" href="https://kavorite.github.io/vault/notes/gj4j8mv891v6ofxgbv7a3fu/"/><meta name="next-head-count" content="14"/><link rel="preload" href="/vault/_next/static/css/bec73badd4e5ba88.css" as="style"/><link rel="stylesheet" href="/vault/_next/static/css/bec73badd4e5ba88.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/vault/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/vault/_next/static/chunks/webpack-3af8c0cde089b23c.js" defer=""></script><script src="/vault/_next/static/chunks/framework-bb5c596eafb42b22.js" defer=""></script><script src="/vault/_next/static/chunks/main-34d019fcedb697b6.js" defer=""></script><script src="/vault/_next/static/chunks/pages/_app-b20c4be9bb651ea8.js" defer=""></script><script src="/vault/_next/static/chunks/826-e0e455fb469c158f.js" defer=""></script><script src="/vault/_next/static/chunks/986-737e5da213076068.js" defer=""></script><script src="/vault/_next/static/chunks/pages/notes/%5Bid%5D-00dd1421f3ce3a3e.js" defer=""></script><script src="/vault/_next/static/x67zoUTAB0zyGOxjfffun/_buildManifest.js" defer=""></script><script src="/vault/_next/static/x67zoUTAB0zyGOxjfffun/_ssgManifest.js" defer=""></script><script src="/vault/_next/static/x67zoUTAB0zyGOxjfffun/_middlewareManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><section class="ant-layout" style="width:100%;min-height:100%"><header class="ant-layout-header" style="position:fixed;isolation:isolate;z-index:1;width:100%;border-bottom:1px solid #d4dadf;height:64px;padding:0 24px 0 2px"><div class="ant-row ant-row-center" style="max-width:992px;justify-content:space-between;margin:0 auto"><div style="display:flex" class="ant-col ant-col-xs-20 ant-col-sm-4"></div><div class="ant-col gutter-row ant-col-xs-0 ant-col-sm-20 ant-col-md-20 ant-col-lg-19"><div class="ant-select ant-select-lg ant-select-auto-complete ant-select-single ant-select-allow-clear ant-select-show-search" style="width:100%"><div class="ant-select-selector"><span class="ant-select-selection-search"><input type="search" autoComplete="off" class="ant-select-selection-search-input" role="combobox" aria-haspopup="listbox" aria-owns="undefined_list" aria-autocomplete="list" aria-controls="undefined_list" aria-activedescendant="undefined_list_0" value=""/></span><span class="ant-select-selection-placeholder">For full text search please use the &#x27;?&#x27; prefix. e.g. ? Onboarding</span></div></div></div><div style="display:none;align-items:center;justify-content:center" class="ant-col ant-col-xs-4 ant-col-sm-4 ant-col-md-0 ant-col-lg-0"><span role="img" aria-label="menu" style="font-size:24px" tabindex="-1" class="anticon anticon-menu"><svg viewBox="64 64 896 896" focusable="false" data-icon="menu" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M904 160H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0 624H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0-312H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8z"></path></svg></span></div></div></header><section class="ant-layout" style="margin-top:64px;display:flex;flex-direction:row"><div class="site-layout-sidebar" style="flex:0 0 auto;width:calc(max((100% - 992px) / 2, 0px) + 200px);min-width:200px;padding-left:calc((100% - 992px) / 2)"><aside class="ant-layout-sider ant-layout-sider-dark" style="position:fixed;overflow:auto;height:calc(100vh - 64px);background-color:transparent;flex:0 0 200px;max-width:200px;min-width:200px;width:200px"><div class="ant-layout-sider-children"></div></aside></div><section class="ant-layout side-layout-main" style="max-width:1200px;display:initial"><main class="ant-layout-content main-content" role="main" style="padding:0 24px"><div class="ant-row"><div class="ant-col ant-col-24"><div class="ant-row" style="margin-left:-10px;margin-right:-10px"><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-24 ant-col-md-18"><div><h1 id="structured-state-space-sequence-models"><a aria-hidden="true" class="anchor-heading icon-link" href="#structured-state-space-sequence-models"></a>Structured State Space Sequence Models</h1>
<p><a href="https://arxiv.org/abs/2111.00396">S4</a> stands for "Efficiently Modeling Long Sequences with Structured State Spaces," an acronym coined in its publication of the same name by <a href="https://arxiv.org/abs/2111.00396">Gu et al., 2021.</a> This paper uses a continuous-time parametrization to model long-range dependencies in terms of multiple independent linear state space models in 1D sequences: But more importantly, the authors derive a practical optimization framework for such models that enables trivially data-parallel training and causal, autoregressive inference in <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord">1</span><span class="mclose">)</span></span></span></span></span> space by discretizing this continuous-time parametrization on-the-fly.</p>
<p>Causal convolutional kernels the same width as the input signal <em>at arbitrary timescales</em> are useful for fitting the model to signals causally during optimization, but alternatively, the underlying parameterization can be instead discretized as parameters for a <em>polynomial recurrence model</em> that explicitly stores the underlying "state" of the system between "transitions" (e.g., HiPPO kernels) between applications of the underlying discrete update step, or "state transitions—" allowing reconstruction of past context not only theoretically but <em>empirically.</em> This allows the model not only to scale log-linearly in the parameter count necessary to model the sequence length that it is expected to encounter. Deep SSMs provide a very promising alternative to transformer architectures, particularly for irregularly-sampled signals or extremely long contexts.</p>
<p><img src="/vault/assets/images/s4.png" alt="S4"></p>
<p>Offshoots include <a href="https://arxiv.org/abs/2206.11893">S4D</a>. Gu et al. found that "restricting the [state transition matrix] to be fully diagonal can still preserve the performance of the original model." Empirically, this works best when the continuous-time parametrization of S4 is left at the mercy of a discrete input signal which does not obey so many temporally-correlated signals as to be encapsulated with only one state space model: I.e., rather than the problem requiring reconstruction of continuous patterns, it merely requires reconstructing a specific subset of discrete signals. Slimming down the intermediate S4 module is thereby sometimes empirically observed to have better convergence properties and make better use of the same number of parameters on language tasks. <a href="https://arxiv.org/abs/2206.13947">Mehta, et al., 2022</a> found that a learnable parameter, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Δ</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\Delta,</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord">Δ</span><span class="mpunct">,</span></span></span></span></span> could even be completely fixed (set to 1) in their experiments without impacting performance— although they did acknowledge that they trained on relatively shorter sequence lengths, and used vastly more compute. </p>
<p><img src="/vault/assets/images/s4d.png" alt="S4D"></p>
<h3 id="arbitrary-temporal-masking"><a aria-hidden="true" class="anchor-heading icon-link" href="#arbitrary-temporal-masking"></a>Arbitrary temporal masking</h3>
<p>For self-attention models, it's trivial to prevent models from erroneously learning relationships between or within arbitrary points in a temporal sequence by adding a large bias to their intermediate attention logits, directly modifying the pointwise mutual information available to the model during training.</p>
<p><img src="/vault/assets/images/diag_mask.png" alt="Shin, et al., 2020."></p>
<p>Preventing attention <em>between</em> samples that occur in the same position within a minibatch using a method like <a href="https://arxiv.org/abs/2107.02027">SPFHP</a> (Kosec, et al., 2021) may be possible with S4 by concatenating convolutional kernels sampled for two different temporal signals. It isn't as obvious how to prevent the model from erroneously learning relationships between points <em>within</em> a sequence for which only one temporal kernel of uniform width can be computed, however, which may entail a need to compute multiple kernels of limited contextual length in order to perform arbitrary masking between points within a sequence (obviously not ideal).</p>
<p>Although <a href="/vault/notes/1rm9i6wwcy7k2rkioy52rs3">Language Autoencoding</a> was originally intended for attention models such as transformers and their derivatives— which enjoy great ease of implementation for this method because of their explicit representation of the pointwise mutual information between discrete points within a given sequence— In principle, it could work for arbitrary sequence models if there were some way to "block" arbitrary points within a given sequence from influencing one another on a forward pass in the same way. For something like S4, the solution does not immediately present itself, although the problem of preventing such models from erroneously finding relationships <em>between</em> samples within a forward pass seems less challenging, as one may simply choose to concatenate two convolutional kernels the same width as the respective temporal signals. </p>
<hr>
<strong>Backlinks</strong>
<ul>
<li><a href="/vault/notes/hwm8yedokqpoldsx5rsjghh">TIRR (vault)</a></li>
</ul></div></div><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-0 ant-col-md-6"><div><div class=""><div class="ant-anchor-wrapper dendron-toc" style="max-height:calc(100vh - 64px);z-index:1"><div class="ant-anchor"><div class="ant-anchor-ink"><span class="ant-anchor-ink-ball"></span></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#arbitrary-temporal-masking" title="Arbitrary temporal masking">Arbitrary temporal masking</a></div></div></div></div></div></div></div></div></div></main><div class="ant-divider ant-divider-horizontal" role="separator"></div><footer class="ant-layout-footer" style="padding:0 24px 24px"></footer></section></section></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note":{"id":"gj4j8mv891v6ofxgbv7a3fu","title":"Structured State Space Sequence Models","desc":"","updated":1659482321428,"created":1658838231340,"custom":{},"fname":"ref.arch.s4","type":"note","vault":{"fsPath":"vault"},"contentHash":"58b339c4678a4555d6cf3f694b7d8b5f","links":[{"type":"wiki","from":{"fname":"ref.arch.s4","id":"gj4j8mv891v6ofxgbv7a3fu","vaultName":"vault"},"value":"ref.nlp.lae","alias":"ref.nlp.lae","position":{"start":{"line":21,"column":10,"offset":3571},"end":{"line":21,"column":25,"offset":3586},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"ref.nlp.lae"}},{"from":{"fname":"ref.cl.tirr","vaultName":"vault"},"type":"backlink","position":{"start":{"line":8,"column":365,"offset":1377},"end":{"line":8,"column":380,"offset":1392},"indent":[]},"value":"ref.arch.s4","alias":"ref.arch.s4"}],"anchors":{"arbitrary-temporal-masking":{"type":"header","text":"Arbitrary temporal masking","value":"arbitrary-temporal-masking","line":19,"column":0,"depth":3}},"children":[],"parent":"39acu8nb3xn6t18xul0ji8d","data":{}},"body":"\u003ch1 id=\"structured-state-space-sequence-models\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#structured-state-space-sequence-models\"\u003e\u003c/a\u003eStructured State Space Sequence Models\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2111.00396\"\u003eS4\u003c/a\u003e stands for \"Efficiently Modeling Long Sequences with Structured State Spaces,\" an acronym coined in its publication of the same name by \u003ca href=\"https://arxiv.org/abs/2111.00396\"\u003eGu et al., 2021.\u003c/a\u003e This paper uses a continuous-time parametrization to model long-range dependencies in terms of multiple independent linear state space models in 1D sequences: But more importantly, the authors derive a practical optimization framework for such models that enables trivially data-parallel training and causal, autoregressive inference in \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eO\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eO(1)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\"\u003eO\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord\"\u003e1\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e space by discretizing this continuous-time parametrization on-the-fly.\u003c/p\u003e\n\u003cp\u003eCausal convolutional kernels the same width as the input signal \u003cem\u003eat arbitrary timescales\u003c/em\u003e are useful for fitting the model to signals causally during optimization, but alternatively, the underlying parameterization can be instead discretized as parameters for a \u003cem\u003epolynomial recurrence model\u003c/em\u003e that explicitly stores the underlying \"state\" of the system between \"transitions\" (e.g., HiPPO kernels) between applications of the underlying discrete update step, or \"state transitions—\" allowing reconstruction of past context not only theoretically but \u003cem\u003eempirically.\u003c/em\u003e This allows the model not only to scale log-linearly in the parameter count necessary to model the sequence length that it is expected to encounter. Deep SSMs provide a very promising alternative to transformer architectures, particularly for irregularly-sampled signals or extremely long contexts.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/vault/assets/images/s4.png\" alt=\"S4\"\u003e\u003c/p\u003e\n\u003cp\u003eOffshoots include \u003ca href=\"https://arxiv.org/abs/2206.11893\"\u003eS4D\u003c/a\u003e. Gu et al. found that \"restricting the [state transition matrix] to be fully diagonal can still preserve the performance of the original model.\" Empirically, this works best when the continuous-time parametrization of S4 is left at the mercy of a discrete input signal which does not obey so many temporally-correlated signals as to be encapsulated with only one state space model: I.e., rather than the problem requiring reconstruction of continuous patterns, it merely requires reconstructing a specific subset of discrete signals. Slimming down the intermediate S4 module is thereby sometimes empirically observed to have better convergence properties and make better use of the same number of parameters on language tasks. \u003ca href=\"https://arxiv.org/abs/2206.13947\"\u003eMehta, et al., 2022\u003c/a\u003e found that a learnable parameter, \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi mathvariant=\"normal\"\u003eΔ\u003c/mi\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\Delta,\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8778em;vertical-align:-0.1944em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003eΔ\u003c/span\u003e\u003cspan class=\"mpunct\"\u003e,\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e could even be completely fixed (set to 1) in their experiments without impacting performance— although they did acknowledge that they trained on relatively shorter sequence lengths, and used vastly more compute. \u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/vault/assets/images/s4d.png\" alt=\"S4D\"\u003e\u003c/p\u003e\n\u003ch3 id=\"arbitrary-temporal-masking\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#arbitrary-temporal-masking\"\u003e\u003c/a\u003eArbitrary temporal masking\u003c/h3\u003e\n\u003cp\u003eFor self-attention models, it's trivial to prevent models from erroneously learning relationships between or within arbitrary points in a temporal sequence by adding a large bias to their intermediate attention logits, directly modifying the pointwise mutual information available to the model during training.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/vault/assets/images/diag_mask.png\" alt=\"Shin, et al., 2020.\"\u003e\u003c/p\u003e\n\u003cp\u003ePreventing attention \u003cem\u003ebetween\u003c/em\u003e samples that occur in the same position within a minibatch using a method like \u003ca href=\"https://arxiv.org/abs/2107.02027\"\u003eSPFHP\u003c/a\u003e (Kosec, et al., 2021) may be possible with S4 by concatenating convolutional kernels sampled for two different temporal signals. It isn't as obvious how to prevent the model from erroneously learning relationships between points \u003cem\u003ewithin\u003c/em\u003e a sequence for which only one temporal kernel of uniform width can be computed, however, which may entail a need to compute multiple kernels of limited contextual length in order to perform arbitrary masking between points within a sequence (obviously not ideal).\u003c/p\u003e\n\u003cp\u003eAlthough \u003ca href=\"/vault/notes/1rm9i6wwcy7k2rkioy52rs3\"\u003eLanguage Autoencoding\u003c/a\u003e was originally intended for attention models such as transformers and their derivatives— which enjoy great ease of implementation for this method because of their explicit representation of the pointwise mutual information between discrete points within a given sequence— In principle, it could work for arbitrary sequence models if there were some way to \"block\" arbitrary points within a given sequence from influencing one another on a forward pass in the same way. For something like S4, the solution does not immediately present itself, although the problem of preventing such models from erroneously finding relationships \u003cem\u003ebetween\u003c/em\u003e samples within a forward pass seems less challenging, as one may simply choose to concatenate two convolutional kernels the same width as the respective temporal signals. \u003c/p\u003e\n\u003chr\u003e\n\u003cstrong\u003eBacklinks\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/vault/notes/hwm8yedokqpoldsx5rsjghh\"\u003eTIRR (vault)\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","noteIndex":{"id":"ihfr21mlvz5lp3n62uu344t","title":"kavlogs","desc":"","updated":1659115143090,"created":1659018913792,"custom":{"nav_order":0,"permalink":"/"},"fname":"ref","type":"note","vault":{"fsPath":"vault"},"contentHash":"41b7e8cfe841b035025dfa85ec0b142e","links":[],"anchors":{},"children":["39acu8nb3xn6t18xul0ji8d","jc56h8njgqawadlroex4ju1","wf5gku64lj2ag6n8kzs9o8f","jlbuz8giyj93jsc1l0lmbzs","3c09ct2zvlk9dfitxexbss3"],"parent":null,"data":{},"body":"This is an assorted collection of notes and thoughts on things I've read about before. Here for my reference, and so that you can come here to read my background instead of receiving an untimely info-dump whose content just amounts to a recitation of anything here, out of mutual respect for my time and yours."},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true,"enableSelfContainedVaults":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":true,"vaultSelectionModeOnCreate":"smart","leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2}},"randomNote":{},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"copyNoteLink":{},"templateHierarchy":"template"},"workspace":{"vaults":[{"fsPath":"vault"},{"fsPath":"projects"},{"fsPath":"tenets"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"task":{"name":"task","dateFormat":"y.MM.dd","addBehavior":"asOwnDomain","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"taskCompleteStatus":["done","x"],"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableHandlebarTemplates":true,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"enableUserTags":true,"enableHashTags":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":false,"enableEditorDecorations":true,"maxPreviewsCached":10,"maxNoteLength":204800,"enableFullHierarchyNoteTitle":false,"enableSmartRefs":true,"dendronVersion":"0.105.1"},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":true},"publishing":{"theme":"dark","enableFMTitle":true,"enableNoteTitleForLink":true,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteUrl":"https://kavorite.github.io","assetsPrefix":"/vault","siteHierarchies":["ref","pub"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"⚑","description":"kavlogs"},"github":{"enableEditLink":false,"editBranch":"main","editViewMode":"tree","editLinkText":"Edit this page on GitHub"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"siteFaviconPath":"favicon.ico","siteIndex":"ref"}}},"__N_SSG":true},"page":"/notes/[id]","query":{"id":"gj4j8mv891v6ofxgbv7a3fu"},"buildId":"x67zoUTAB0zyGOxjfffun","assetPrefix":"/vault","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>
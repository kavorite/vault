<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><link rel="icon" href="/favicon.ico"/><title>Language Autoencoding</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="kavlogs"/><meta property="og:title" content="Language Autoencoding"/><meta property="og:description" content="kavlogs"/><meta property="og:url" content="https://kavorite.github.io/vault/notes/1rm9i6wwcy7k2rkioy52rs3/"/><meta property="og:type" content="article"/><meta property="article:published_time" content="7/27/2022"/><meta property="article:modified_time" content="7/27/2022"/><link rel="canonical" href="https://kavorite.github.io/vault/notes/1rm9i6wwcy7k2rkioy52rs3/"/><meta name="next-head-count" content="14"/><link rel="preload" href="/vault/_next/static/css/bec73badd4e5ba88.css" as="style"/><link rel="stylesheet" href="/vault/_next/static/css/bec73badd4e5ba88.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/vault/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/vault/_next/static/chunks/webpack-3af8c0cde089b23c.js" defer=""></script><script src="/vault/_next/static/chunks/framework-bb5c596eafb42b22.js" defer=""></script><script src="/vault/_next/static/chunks/main-34d019fcedb697b6.js" defer=""></script><script src="/vault/_next/static/chunks/pages/_app-b20c4be9bb651ea8.js" defer=""></script><script src="/vault/_next/static/chunks/826-e0e455fb469c158f.js" defer=""></script><script src="/vault/_next/static/chunks/986-737e5da213076068.js" defer=""></script><script src="/vault/_next/static/chunks/pages/notes/%5Bid%5D-00dd1421f3ce3a3e.js" defer=""></script><script src="/vault/_next/static/x67zoUTAB0zyGOxjfffun/_buildManifest.js" defer=""></script><script src="/vault/_next/static/x67zoUTAB0zyGOxjfffun/_ssgManifest.js" defer=""></script><script src="/vault/_next/static/x67zoUTAB0zyGOxjfffun/_middlewareManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><section class="ant-layout" style="width:100%;min-height:100%"><header class="ant-layout-header" style="position:fixed;isolation:isolate;z-index:1;width:100%;border-bottom:1px solid #d4dadf;height:64px;padding:0 24px 0 2px"><div class="ant-row ant-row-center" style="max-width:992px;justify-content:space-between;margin:0 auto"><div style="display:flex" class="ant-col ant-col-xs-20 ant-col-sm-4"></div><div class="ant-col gutter-row ant-col-xs-0 ant-col-sm-20 ant-col-md-20 ant-col-lg-19"><div class="ant-select ant-select-lg ant-select-auto-complete ant-select-single ant-select-allow-clear ant-select-show-search" style="width:100%"><div class="ant-select-selector"><span class="ant-select-selection-search"><input type="search" autoComplete="off" class="ant-select-selection-search-input" role="combobox" aria-haspopup="listbox" aria-owns="undefined_list" aria-autocomplete="list" aria-controls="undefined_list" aria-activedescendant="undefined_list_0" value=""/></span><span class="ant-select-selection-placeholder">For full text search please use the &#x27;?&#x27; prefix. e.g. ? Onboarding</span></div></div></div><div style="display:none;align-items:center;justify-content:center" class="ant-col ant-col-xs-4 ant-col-sm-4 ant-col-md-0 ant-col-lg-0"><span role="img" aria-label="menu" style="font-size:24px" tabindex="-1" class="anticon anticon-menu"><svg viewBox="64 64 896 896" focusable="false" data-icon="menu" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M904 160H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0 624H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0-312H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8z"></path></svg></span></div></div></header><section class="ant-layout" style="margin-top:64px;display:flex;flex-direction:row"><div class="site-layout-sidebar" style="flex:0 0 auto;width:calc(max((100% - 992px) / 2, 0px) + 200px);min-width:200px;padding-left:calc((100% - 992px) / 2)"><aside class="ant-layout-sider ant-layout-sider-dark" style="position:fixed;overflow:auto;height:calc(100vh - 64px);background-color:transparent;flex:0 0 200px;max-width:200px;min-width:200px;width:200px"><div class="ant-layout-sider-children"></div></aside></div><section class="ant-layout side-layout-main" style="max-width:1200px;display:initial"><main class="ant-layout-content main-content" role="main" style="padding:0 24px"><div class="ant-row"><div class="ant-col ant-col-24"><div class="ant-row" style="margin-left:-10px;margin-right:-10px"><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-24 ant-col-md-18"><div><h1 id="language-autoencoding"><a aria-hidden="true" class="anchor-heading icon-link" href="#language-autoencoding"></a>Language Autoencoding</h1>
<p><strong>L</strong>anguage <strong>A</strong>uto<strong>E</strong>ncoding, or LAE, is a transformer objective proposed by <a href="https://arxiv.org/abs/2004.08097">Shin, et al., 2020</a> for training self-attention mechanisms for bidirectional NLU by explicitly masking out dot-product self-attention matrices with large negative bias on the diagonal, essentially "disabling" the model's internal representation that tokens "attend" to themselves and requiring each token to be predicted from surrounding context. </p>
<p><img src="/vault/assets/images/lae.png" alt="Language Autoencoding, from Shin, et al."></p>
<p>Shin, et al. acknowledge the relatedness of their method to masked language modeling, or MLM, objectives such as the one used to fit <a href="https://arxiv.org/abs/1810.04805">BERT</a> (Devlin et al., 2018), drawing the analogical relationship of autoencoders to denoising autoencoders. I suspect LAE may be a more sample-efficient and smoother objective than BERT, while not requiring the same computational overhead and mind-bending technical considerations as <a href="https://arxiv.org/abs/1906.08237">XLNet</a>, as it requires learning bidirectional representations between all tokens and introduces no incongruities between train- and test-time as BERT does with its <code>[MASK]</code> token.</p>
<p>Self-attention masks could also potentially be used for regularization along alternative methods that explicitly perturb input sequences such as <a href="https://arxiv.org/abs/1906.08101">whole-word masking</a> by being more selective about which positions are "masked" on the diagonal.</p>
<p><img src="/vault/assets/images/diag_mask.png" alt="Diagonal Masking, from Shin, et al."></p>
<hr>
<strong>Backlinks</strong>
<ul>
<li><a href="/vault/notes/gj4j8mv891v6ofxgbv7a3fu">Structured State Space Sequence Models (vault)</a></li>
</ul></div></div><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-0 ant-col-md-6"><div><div class=""><div class="ant-anchor-wrapper dendron-toc" style="max-height:calc(100vh - 64px);z-index:1"><div class="ant-anchor"><div class="ant-anchor-ink"><span class="ant-anchor-ink-ball"></span></div></div></div></div></div></div></div></div></div></main><div class="ant-divider ant-divider-horizontal" role="separator"></div><footer class="ant-layout-footer" style="padding:0 24px 24px"></footer></section></section></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note":{"id":"1rm9i6wwcy7k2rkioy52rs3","title":"Language Autoencoding","desc":"","updated":1658949026452,"created":1658941369199,"custom":{},"fname":"ref.nlp.lae","type":"note","vault":{"fsPath":"vault"},"contentHash":"e0f9b93b0699d46bbc37ab24580a4506","links":[{"from":{"fname":"ref.arch.s4","vaultName":"vault"},"type":"backlink","position":{"start":{"line":21,"column":10,"offset":3571},"end":{"line":21,"column":25,"offset":3586},"indent":[]},"value":"ref.nlp.lae","alias":"ref.nlp.lae"}],"anchors":{},"children":[],"parent":"jlbuz8giyj93jsc1l0lmbzs","data":{}},"body":"\u003ch1 id=\"language-autoencoding\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#language-autoencoding\"\u003e\u003c/a\u003eLanguage Autoencoding\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eL\u003c/strong\u003eanguage \u003cstrong\u003eA\u003c/strong\u003euto\u003cstrong\u003eE\u003c/strong\u003encoding, or LAE, is a transformer objective proposed by \u003ca href=\"https://arxiv.org/abs/2004.08097\"\u003eShin, et al., 2020\u003c/a\u003e for training self-attention mechanisms for bidirectional NLU by explicitly masking out dot-product self-attention matrices with large negative bias on the diagonal, essentially \"disabling\" the model's internal representation that tokens \"attend\" to themselves and requiring each token to be predicted from surrounding context. \u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/vault/assets/images/lae.png\" alt=\"Language Autoencoding, from Shin, et al.\"\u003e\u003c/p\u003e\n\u003cp\u003eShin, et al. acknowledge the relatedness of their method to masked language modeling, or MLM, objectives such as the one used to fit \u003ca href=\"https://arxiv.org/abs/1810.04805\"\u003eBERT\u003c/a\u003e (Devlin et al., 2018), drawing the analogical relationship of autoencoders to denoising autoencoders. I suspect LAE may be a more sample-efficient and smoother objective than BERT, while not requiring the same computational overhead and mind-bending technical considerations as \u003ca href=\"https://arxiv.org/abs/1906.08237\"\u003eXLNet\u003c/a\u003e, as it requires learning bidirectional representations between all tokens and introduces no incongruities between train- and test-time as BERT does with its \u003ccode\u003e[MASK]\u003c/code\u003e token.\u003c/p\u003e\n\u003cp\u003eSelf-attention masks could also potentially be used for regularization along alternative methods that explicitly perturb input sequences such as \u003ca href=\"https://arxiv.org/abs/1906.08101\"\u003ewhole-word masking\u003c/a\u003e by being more selective about which positions are \"masked\" on the diagonal.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/vault/assets/images/diag_mask.png\" alt=\"Diagonal Masking, from Shin, et al.\"\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003cstrong\u003eBacklinks\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/vault/notes/gj4j8mv891v6ofxgbv7a3fu\"\u003eStructured State Space Sequence Models (vault)\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","noteIndex":{"id":"ihfr21mlvz5lp3n62uu344t","title":"kavlogs","desc":"","updated":1659115143090,"created":1659018913792,"custom":{"nav_order":0,"permalink":"/"},"fname":"ref","type":"note","vault":{"fsPath":"vault"},"contentHash":"41b7e8cfe841b035025dfa85ec0b142e","links":[],"anchors":{},"children":["39acu8nb3xn6t18xul0ji8d","jc56h8njgqawadlroex4ju1","wf5gku64lj2ag6n8kzs9o8f","jlbuz8giyj93jsc1l0lmbzs","3c09ct2zvlk9dfitxexbss3"],"parent":null,"data":{},"body":"This is an assorted collection of notes and thoughts on things I've read about before. Here for my reference, and so that you can come here to read my background instead of receiving an untimely info-dump whose content just amounts to a recitation of anything here, out of mutual respect for my time and yours."},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true,"enableSelfContainedVaults":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":true,"vaultSelectionModeOnCreate":"smart","leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2}},"randomNote":{},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"copyNoteLink":{},"templateHierarchy":"template"},"workspace":{"vaults":[{"fsPath":"vault"},{"fsPath":"projects"},{"fsPath":"tenets"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"task":{"name":"task","dateFormat":"y.MM.dd","addBehavior":"asOwnDomain","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"taskCompleteStatus":["done","x"],"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableHandlebarTemplates":true,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"enableUserTags":true,"enableHashTags":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":false,"enableEditorDecorations":true,"maxPreviewsCached":10,"maxNoteLength":204800,"enableFullHierarchyNoteTitle":false,"enableSmartRefs":true,"dendronVersion":"0.105.1"},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":true},"publishing":{"theme":"dark","enableFMTitle":true,"enableNoteTitleForLink":true,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteUrl":"https://kavorite.github.io","assetsPrefix":"/vault","siteHierarchies":["ref","pub"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"âš‘","description":"kavlogs"},"github":{"enableEditLink":false,"editBranch":"main","editViewMode":"tree","editLinkText":"Edit this page on GitHub"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"siteFaviconPath":"favicon.ico","siteIndex":"ref"}}},"__N_SSG":true},"page":"/notes/[id]","query":{"id":"1rm9i6wwcy7k2rkioy52rs3"},"buildId":"x67zoUTAB0zyGOxjfffun","assetPrefix":"/vault","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>
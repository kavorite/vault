<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><link rel="icon" href="/favicon.ico"/><title>DEQ Extensions</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="kavlogs"/><meta property="og:title" content="DEQ Extensions"/><meta property="og:description" content="kavlogs"/><meta property="og:url" content="https://kavorite.github.io/vault/notes/txmy1anxf7bhr607yyk7te7/"/><meta property="og:type" content="article"/><meta property="article:published_time" content="7/27/2022"/><meta property="article:modified_time" content="8/4/2022"/><link rel="canonical" href="https://kavorite.github.io/vault/notes/txmy1anxf7bhr607yyk7te7/"/><meta name="next-head-count" content="14"/><link rel="preload" href="/vault/_next/static/css/bec73badd4e5ba88.css" as="style"/><link rel="stylesheet" href="/vault/_next/static/css/bec73badd4e5ba88.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/vault/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/vault/_next/static/chunks/webpack-3af8c0cde089b23c.js" defer=""></script><script src="/vault/_next/static/chunks/framework-bb5c596eafb42b22.js" defer=""></script><script src="/vault/_next/static/chunks/main-34d019fcedb697b6.js" defer=""></script><script src="/vault/_next/static/chunks/pages/_app-b20c4be9bb651ea8.js" defer=""></script><script src="/vault/_next/static/chunks/826-e0e455fb469c158f.js" defer=""></script><script src="/vault/_next/static/chunks/986-737e5da213076068.js" defer=""></script><script src="/vault/_next/static/chunks/pages/notes/%5Bid%5D-00dd1421f3ce3a3e.js" defer=""></script><script src="/vault/_next/static/x67zoUTAB0zyGOxjfffun/_buildManifest.js" defer=""></script><script src="/vault/_next/static/x67zoUTAB0zyGOxjfffun/_ssgManifest.js" defer=""></script><script src="/vault/_next/static/x67zoUTAB0zyGOxjfffun/_middlewareManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><section class="ant-layout" style="width:100%;min-height:100%"><header class="ant-layout-header" style="position:fixed;isolation:isolate;z-index:1;width:100%;border-bottom:1px solid #d4dadf;height:64px;padding:0 24px 0 2px"><div class="ant-row ant-row-center" style="max-width:992px;justify-content:space-between;margin:0 auto"><div style="display:flex" class="ant-col ant-col-xs-20 ant-col-sm-4"></div><div class="ant-col gutter-row ant-col-xs-0 ant-col-sm-20 ant-col-md-20 ant-col-lg-19"><div class="ant-select ant-select-lg ant-select-auto-complete ant-select-single ant-select-allow-clear ant-select-show-search" style="width:100%"><div class="ant-select-selector"><span class="ant-select-selection-search"><input type="search" autoComplete="off" class="ant-select-selection-search-input" role="combobox" aria-haspopup="listbox" aria-owns="undefined_list" aria-autocomplete="list" aria-controls="undefined_list" aria-activedescendant="undefined_list_0" value=""/></span><span class="ant-select-selection-placeholder">For full text search please use the &#x27;?&#x27; prefix. e.g. ? Onboarding</span></div></div></div><div style="display:none;align-items:center;justify-content:center" class="ant-col ant-col-xs-4 ant-col-sm-4 ant-col-md-0 ant-col-lg-0"><span role="img" aria-label="menu" style="font-size:24px" tabindex="-1" class="anticon anticon-menu"><svg viewBox="64 64 896 896" focusable="false" data-icon="menu" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M904 160H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0 624H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0-312H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8z"></path></svg></span></div></div></header><section class="ant-layout" style="margin-top:64px;display:flex;flex-direction:row"><div class="site-layout-sidebar" style="flex:0 0 auto;width:calc(max((100% - 992px) / 2, 0px) + 200px);min-width:200px;padding-left:calc((100% - 992px) / 2)"><aside class="ant-layout-sider ant-layout-sider-dark" style="position:fixed;overflow:auto;height:calc(100vh - 64px);background-color:transparent;flex:0 0 200px;max-width:200px;min-width:200px;width:200px"><div class="ant-layout-sider-children"></div></aside></div><section class="ant-layout side-layout-main" style="max-width:1200px;display:initial"><main class="ant-layout-content main-content" role="main" style="padding:0 24px"><div class="ant-row"><div class="ant-col ant-col-24"><div class="ant-row" style="margin-left:-10px;margin-right:-10px"><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-24 ant-col-md-18"><div><h1 id="deq-extensions"><a aria-hidden="true" class="anchor-heading icon-link" href="#deq-extensions"></a>DEQ Extensions</h1>
<h2 id="jacobian-regularization"><a aria-hidden="true" class="anchor-heading icon-link" href="#jacobian-regularization"></a><a href="http://implicit-layers-tutorial.org/deep_equilibrium_models/">Jacobian Regularization</a></h2>
<p>DEQs turned out not to work especially well in practice, learning hard-to-solve Jacobian matrices, which caused training to lose stability and become less computationally tractable over time. Bai et al., the same research group which invented DEQs, later presented methods that allow this behavior to be explicitly regularized, bringing their computational costs to par with those of explicit networks.</p>
<h2 id="skipdeq"><a aria-hidden="true" class="anchor-heading icon-link" href="#skipdeq"></a><a href="https://arxiv.org/abs/2201.12240">SkipDEQ</a></h2>
<p>Introduced by Pal, et al., 2022, <a href="https://arxiv.org/abs/2201.12240">SkipDEQs</a> combine DEQs with an explicit correction layer which conditions them on their inputs and predicts initial hidden states, not unlike <a href="/vault/notes/zy4oqk1z59whxqbjtmwbdm4">Feature-wise Linear Modulation</a>, stabilizing and expediting training. It is <a href="https://github.com/SciML/DeepEquilibriumNetworks.jl">implemented</a> in Julia. Based on the difficulties that vanilla DEQs have with matching the computational efficiency of deep explicit models, all practical implementations of DEQs should probably look something like this.</p>
<h2 id="mdeq"><a aria-hidden="true" class="anchor-heading icon-link" href="#mdeq"></a><a href="https://arxiv.org/abs/2006.08656">MDEQ</a></h2>
<p>I'm somewhat unimpressed with MDEQ. I don't think that downsampling to multiple resolutions at the input with weight-tying is the best way to exploit relationships between different feature scales. Resampling the input just so that it can be passed through the same DEQ cell layer multiple times in parallel results in loss of some simplicity compared to baseline explicit methods such as vanilla ResNets, which can just use intermediate downsampling during forward propagation. </p>
<p>MDEQ's construction is motivated by an observation of poor generalization performance between multiple feature scales in discrete convolutions in an environment where applications of convolutions are dominated by tasks where scale invariance is highly desirable, leading <a href="https://arxiv.org/abs/1512.03385">He, et al., 2015</a> and many others to perform intermediate downsampling, introducing explicitly hierarchical patterns to how spatially correlated inputs are processed. Aggressive downsampling leads both to considerable improvements in both computational tractability and feature reuse: A success replicated many times over both in discriminative and generative image processing applications, including by UNets, proposed by <a href="https://arxiv.org/abs/1505.04597">Ronneberger, et al., 2015</a>, NVAE, by <a href="https://arxiv.org/abs/2007.03898">Vahdat and Kautz, 2020</a>, and <a href="/vault/notes/s2dvzt8effhbjxgsflxlw75">UnCLIP</a>, by Ramesh, et al., 2022.</p>
<p>On that basis, one might replace <a href="https://arxiv.org/abs/2006.08656">MDEQ</a>'s approach of backpropagating through multiple predefined feature resolutions simultaneously with something more akin to a <a href="/vault/notes/kxklakzrojw3q2r808mzh65">FlexConv</a>, instead, which takes the analogous approach of simply broadening the receptive field with increasing depth using continuous kernels that boast a fixed parameter cost, or even explicitly parametrize over the spatial downsampling factor at each step, while constraining resolution to lie above some fixed threshold, reducing the number of hyperparameters required for intermediate pooling while keeping most, if not all of the resulting representational capacity and computational dividends.</p>
<h4 id="sam"><a aria-hidden="true" class="anchor-heading icon-link" href="#sam"></a><a href="/vault/notes/3cublmnyywkz5grltuf1253">SAM</a></h4>
<p>Other forms of explicit regularization might also be of use in combination with explicit regularization of the Jacobian: I'm not sure SAM has ever been applied in tandem with implicit differentiation, but I'm also not sure why it wouldn't work in principle. As noted by Bai, et al., more conventional regularization techniques such as weight standardization and feature- and batch-wise normalization are critical for convergence of "vanilla" DEQs in practice, pointing to a relationship between the explicit "differentiability" of deep neural networks when taken as fixed-iteration equilibrium solvers, and the ease of explicitly solving for points of equilibrium in DEQs. I would trade compute off for reliability, hyperparameter-stability, and sample efficiency any day of the week, implicit backpropagation or not. Perhaps I should more deeply explore whether this is possible. </p>
<hr>
<strong>Backlinks</strong>
<ul>
<li><a href="/vault/notes/vl3h77sg0urrzngvpa94cp0">Deep Equilibrium Models (vault)</a></li>
</ul></div></div><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-0 ant-col-md-6"><div><div class=""><div class="ant-anchor-wrapper dendron-toc" style="max-height:calc(100vh - 64px);z-index:1"><div class="ant-anchor"><div class="ant-anchor-ink"><span class="ant-anchor-ink-ball"></span></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#jacobian-regularization" title="Jacobian Regularization">Jacobian Regularization</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#skipdeq" title="SkipDEQ">SkipDEQ</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#mdeq" title="MDEQ">MDEQ</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#sam" title="SAM">SAM</a></div></div></div></div></div></div></div></div></div></main><div class="ant-divider ant-divider-horizontal" role="separator"></div><footer class="ant-layout-footer" style="padding:0 24px 24px"></footer></section></section></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note":{"id":"txmy1anxf7bhr607yyk7te7","title":"DEQ Extensions","desc":"","updated":1659577648992,"created":1658945653478,"custom":{},"fname":"ref.arch.deq.extensions","type":"note","vault":{"fsPath":"vault"},"contentHash":"0ba934b7a1c3aa81128c1c5d9ee507be","links":[{"type":"wiki","from":{"fname":"ref.arch.deq.extensions","id":"txmy1anxf7bhr607yyk7te7","vaultName":"vault"},"value":"ref.opt.film","alias":"ref.opt.film","position":{"start":{"line":6,"column":186,"offset":649},"end":{"line":6,"column":202,"offset":665},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"ref.opt.film"}},{"type":"wiki","from":{"fname":"ref.arch.deq.extensions","id":"txmy1anxf7bhr607yyk7te7","vaultName":"vault"},"value":"ref.cv.unclip","alias":"ref.cv.unclip","position":{"start":{"line":12,"column":786,"offset":2223},"end":{"line":12,"column":803,"offset":2240},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"ref.cv.unclip"}},{"type":"wiki","from":{"fname":"ref.arch.deq.extensions","id":"txmy1anxf7bhr607yyk7te7","vaultName":"vault"},"value":"ref.arch.ckconv","alias":"FlexConv","position":{"start":{"line":14,"column":164,"offset":2431},"end":{"line":14,"column":192,"offset":2459},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"ref.arch.ckconv"}},{"type":"wiki","from":{"fname":"ref.arch.deq.extensions","id":"txmy1anxf7bhr607yyk7te7","vaultName":"vault"},"value":"ref.opt.sam","alias":"SAM","position":{"start":{"line":16,"column":6,"offset":2957},"end":{"line":16,"column":25,"offset":2976},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"ref.opt.sam"}},{"from":{"fname":"ref.arch.deq","vaultName":"vault"},"type":"backlink","position":{"start":{"line":7,"column":378,"offset":2854},"end":{"line":7,"column":422,"offset":2898},"indent":[]},"value":"ref.arch.deq.extensions","alias":"some adjustments"}],"anchors":{"jacobian-regularization":{"type":"header","text":"Jacobian Regularization","value":"jacobian-regularization","line":8,"column":0,"depth":2},"skipdeq":{"type":"header","text":"SkipDEQ","value":"skipdeq","line":11,"column":0,"depth":2},"mdeq":{"type":"header","text":"MDEQ","value":"mdeq","line":14,"column":0,"depth":2},"sam":{"type":"header","text":"SAM","value":"sam","line":22,"column":0,"depth":4}},"children":[],"parent":"vl3h77sg0urrzngvpa94cp0","data":{}},"body":"\u003ch1 id=\"deq-extensions\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#deq-extensions\"\u003e\u003c/a\u003eDEQ Extensions\u003c/h1\u003e\n\u003ch2 id=\"jacobian-regularization\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#jacobian-regularization\"\u003e\u003c/a\u003e\u003ca href=\"http://implicit-layers-tutorial.org/deep_equilibrium_models/\"\u003eJacobian Regularization\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eDEQs turned out not to work especially well in practice, learning hard-to-solve Jacobian matrices, which caused training to lose stability and become less computationally tractable over time. Bai et al., the same research group which invented DEQs, later presented methods that allow this behavior to be explicitly regularized, bringing their computational costs to par with those of explicit networks.\u003c/p\u003e\n\u003ch2 id=\"skipdeq\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#skipdeq\"\u003e\u003c/a\u003e\u003ca href=\"https://arxiv.org/abs/2201.12240\"\u003eSkipDEQ\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eIntroduced by Pal, et al., 2022, \u003ca href=\"https://arxiv.org/abs/2201.12240\"\u003eSkipDEQs\u003c/a\u003e combine DEQs with an explicit correction layer which conditions them on their inputs and predicts initial hidden states, not unlike \u003ca href=\"/vault/notes/zy4oqk1z59whxqbjtmwbdm4\"\u003eFeature-wise Linear Modulation\u003c/a\u003e, stabilizing and expediting training. It is \u003ca href=\"https://github.com/SciML/DeepEquilibriumNetworks.jl\"\u003eimplemented\u003c/a\u003e in Julia. Based on the difficulties that vanilla DEQs have with matching the computational efficiency of deep explicit models, all practical implementations of DEQs should probably look something like this.\u003c/p\u003e\n\u003ch2 id=\"mdeq\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#mdeq\"\u003e\u003c/a\u003e\u003ca href=\"https://arxiv.org/abs/2006.08656\"\u003eMDEQ\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eI'm somewhat unimpressed with MDEQ. I don't think that downsampling to multiple resolutions at the input with weight-tying is the best way to exploit relationships between different feature scales. Resampling the input just so that it can be passed through the same DEQ cell layer multiple times in parallel results in loss of some simplicity compared to baseline explicit methods such as vanilla ResNets, which can just use intermediate downsampling during forward propagation. \u003c/p\u003e\n\u003cp\u003eMDEQ's construction is motivated by an observation of poor generalization performance between multiple feature scales in discrete convolutions in an environment where applications of convolutions are dominated by tasks where scale invariance is highly desirable, leading \u003ca href=\"https://arxiv.org/abs/1512.03385\"\u003eHe, et al., 2015\u003c/a\u003e and many others to perform intermediate downsampling, introducing explicitly hierarchical patterns to how spatially correlated inputs are processed. Aggressive downsampling leads both to considerable improvements in both computational tractability and feature reuse: A success replicated many times over both in discriminative and generative image processing applications, including by UNets, proposed by \u003ca href=\"https://arxiv.org/abs/1505.04597\"\u003eRonneberger, et al., 2015\u003c/a\u003e, NVAE, by \u003ca href=\"https://arxiv.org/abs/2007.03898\"\u003eVahdat and Kautz, 2020\u003c/a\u003e, and \u003ca href=\"/vault/notes/s2dvzt8effhbjxgsflxlw75\"\u003eUnCLIP\u003c/a\u003e, by Ramesh, et al., 2022.\u003c/p\u003e\n\u003cp\u003eOn that basis, one might replace \u003ca href=\"https://arxiv.org/abs/2006.08656\"\u003eMDEQ\u003c/a\u003e's approach of backpropagating through multiple predefined feature resolutions simultaneously with something more akin to a \u003ca href=\"/vault/notes/kxklakzrojw3q2r808mzh65\"\u003eFlexConv\u003c/a\u003e, instead, which takes the analogous approach of simply broadening the receptive field with increasing depth using continuous kernels that boast a fixed parameter cost, or even explicitly parametrize over the spatial downsampling factor at each step, while constraining resolution to lie above some fixed threshold, reducing the number of hyperparameters required for intermediate pooling while keeping most, if not all of the resulting representational capacity and computational dividends.\u003c/p\u003e\n\u003ch4 id=\"sam\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#sam\"\u003e\u003c/a\u003e\u003ca href=\"/vault/notes/3cublmnyywkz5grltuf1253\"\u003eSAM\u003c/a\u003e\u003c/h4\u003e\n\u003cp\u003eOther forms of explicit regularization might also be of use in combination with explicit regularization of the Jacobian: I'm not sure SAM has ever been applied in tandem with implicit differentiation, but I'm also not sure why it wouldn't work in principle. As noted by Bai, et al., more conventional regularization techniques such as weight standardization and feature- and batch-wise normalization are critical for convergence of \"vanilla\" DEQs in practice, pointing to a relationship between the explicit \"differentiability\" of deep neural networks when taken as fixed-iteration equilibrium solvers, and the ease of explicitly solving for points of equilibrium in DEQs. I would trade compute off for reliability, hyperparameter-stability, and sample efficiency any day of the week, implicit backpropagation or not. Perhaps I should more deeply explore whether this is possible. \u003c/p\u003e\n\u003chr\u003e\n\u003cstrong\u003eBacklinks\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/vault/notes/vl3h77sg0urrzngvpa94cp0\"\u003eDeep Equilibrium Models (vault)\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","noteIndex":{"id":"ihfr21mlvz5lp3n62uu344t","title":"kavlogs","desc":"","updated":1659115143090,"created":1659018913792,"custom":{"nav_order":0,"permalink":"/"},"fname":"ref","type":"note","vault":{"fsPath":"vault"},"contentHash":"41b7e8cfe841b035025dfa85ec0b142e","links":[],"anchors":{},"children":["39acu8nb3xn6t18xul0ji8d","jc56h8njgqawadlroex4ju1","wf5gku64lj2ag6n8kzs9o8f","jlbuz8giyj93jsc1l0lmbzs","3c09ct2zvlk9dfitxexbss3"],"parent":null,"data":{},"body":"This is an assorted collection of notes and thoughts on things I've read about before. Here for my reference, and so that you can come here to read my background instead of receiving an untimely info-dump whose content just amounts to a recitation of anything here, out of mutual respect for my time and yours."},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true,"enableSelfContainedVaults":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":true,"vaultSelectionModeOnCreate":"smart","leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2}},"randomNote":{},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"copyNoteLink":{},"templateHierarchy":"template"},"workspace":{"vaults":[{"fsPath":"vault"},{"fsPath":"projects"},{"fsPath":"tenets"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"task":{"name":"task","dateFormat":"y.MM.dd","addBehavior":"asOwnDomain","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"taskCompleteStatus":["done","x"],"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableHandlebarTemplates":true,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"enableUserTags":true,"enableHashTags":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":false,"enableEditorDecorations":true,"maxPreviewsCached":10,"maxNoteLength":204800,"enableFullHierarchyNoteTitle":false,"enableSmartRefs":true,"dendronVersion":"0.105.1"},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":true},"publishing":{"theme":"dark","enableFMTitle":true,"enableNoteTitleForLink":true,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteUrl":"https://kavorite.github.io","assetsPrefix":"/vault","siteHierarchies":["ref","pub"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"⚑","description":"kavlogs"},"github":{"enableEditLink":false,"editBranch":"main","editViewMode":"tree","editLinkText":"Edit this page on GitHub"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"siteFaviconPath":"favicon.ico","siteIndex":"ref"}}},"__N_SSG":true},"page":"/notes/[id]","query":{"id":"txmy1anxf7bhr607yyk7te7"},"buildId":"x67zoUTAB0zyGOxjfffun","assetPrefix":"/vault","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>
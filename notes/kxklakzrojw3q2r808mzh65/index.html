<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><link rel="icon" href="/favicon.ico"/><title>Continuous Kernel Convolution</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="kavlogs"/><meta property="og:title" content="Continuous Kernel Convolution"/><meta property="og:description" content="kavlogs"/><meta property="og:url" content="https://kavorite.github.io/vault/notes/kxklakzrojw3q2r808mzh65/"/><meta property="og:type" content="article"/><meta property="article:published_time" content="7/26/2022"/><meta property="article:modified_time" content="7/28/2022"/><link rel="canonical" href="https://kavorite.github.io/vault/notes/kxklakzrojw3q2r808mzh65/"/><meta name="next-head-count" content="14"/><link rel="preload" href="/vault/_next/static/css/bec73badd4e5ba88.css" as="style"/><link rel="stylesheet" href="/vault/_next/static/css/bec73badd4e5ba88.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/vault/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/vault/_next/static/chunks/webpack-3af8c0cde089b23c.js" defer=""></script><script src="/vault/_next/static/chunks/framework-bb5c596eafb42b22.js" defer=""></script><script src="/vault/_next/static/chunks/main-34d019fcedb697b6.js" defer=""></script><script src="/vault/_next/static/chunks/pages/_app-b20c4be9bb651ea8.js" defer=""></script><script src="/vault/_next/static/chunks/826-e0e455fb469c158f.js" defer=""></script><script src="/vault/_next/static/chunks/986-737e5da213076068.js" defer=""></script><script src="/vault/_next/static/chunks/pages/notes/%5Bid%5D-00dd1421f3ce3a3e.js" defer=""></script><script src="/vault/_next/static/x67zoUTAB0zyGOxjfffun/_buildManifest.js" defer=""></script><script src="/vault/_next/static/x67zoUTAB0zyGOxjfffun/_ssgManifest.js" defer=""></script><script src="/vault/_next/static/x67zoUTAB0zyGOxjfffun/_middlewareManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><section class="ant-layout" style="width:100%;min-height:100%"><header class="ant-layout-header" style="position:fixed;isolation:isolate;z-index:1;width:100%;border-bottom:1px solid #d4dadf;height:64px;padding:0 24px 0 2px"><div class="ant-row ant-row-center" style="max-width:992px;justify-content:space-between;margin:0 auto"><div style="display:flex" class="ant-col ant-col-xs-20 ant-col-sm-4"></div><div class="ant-col gutter-row ant-col-xs-0 ant-col-sm-20 ant-col-md-20 ant-col-lg-19"><div class="ant-select ant-select-lg ant-select-auto-complete ant-select-single ant-select-allow-clear ant-select-show-search" style="width:100%"><div class="ant-select-selector"><span class="ant-select-selection-search"><input type="search" autoComplete="off" class="ant-select-selection-search-input" role="combobox" aria-haspopup="listbox" aria-owns="undefined_list" aria-autocomplete="list" aria-controls="undefined_list" aria-activedescendant="undefined_list_0" value=""/></span><span class="ant-select-selection-placeholder">For full text search please use the &#x27;?&#x27; prefix. e.g. ? Onboarding</span></div></div></div><div style="display:none;align-items:center;justify-content:center" class="ant-col ant-col-xs-4 ant-col-sm-4 ant-col-md-0 ant-col-lg-0"><span role="img" aria-label="menu" style="font-size:24px" tabindex="-1" class="anticon anticon-menu"><svg viewBox="64 64 896 896" focusable="false" data-icon="menu" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M904 160H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0 624H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0-312H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8z"></path></svg></span></div></div></header><section class="ant-layout" style="margin-top:64px;display:flex;flex-direction:row"><div class="site-layout-sidebar" style="flex:0 0 auto;width:calc(max((100% - 992px) / 2, 0px) + 200px);min-width:200px;padding-left:calc((100% - 992px) / 2)"><aside class="ant-layout-sider ant-layout-sider-dark" style="position:fixed;overflow:auto;height:calc(100vh - 64px);background-color:transparent;flex:0 0 200px;max-width:200px;min-width:200px;width:200px"><div class="ant-layout-sider-children"></div></aside></div><section class="ant-layout side-layout-main" style="max-width:1200px;display:initial"><main class="ant-layout-content main-content" role="main" style="padding:0 24px"><div class="ant-row"><div class="ant-col ant-col-24"><div class="ant-row" style="margin-left:-10px;margin-right:-10px"><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-24 ant-col-md-18"><div><h1 id="continuous-kernel-convolution"><a aria-hidden="true" class="anchor-heading icon-link" href="#continuous-kernel-convolution"></a>Continuous Kernel Convolution</h1>
<p>Continuous-kernel convolutions are just that: Convolutions with a continuous kernel. The kernel is determined by a <strong>generator network</strong> with a fixed parameter cost, by composing a learned linear operator with a predetermined gaussian mask, such as a blur filter, or an (optionally steerable) Gabor filter as in <a href="https://arxiv.org/abs/2110.08059">Romero, et al., 2021's</a> <strong>M</strong>ultiplicative <strong>A</strong>nisotropic <strong>G</strong>abor <strong>N</strong>etworks, or MAGNets.</p>
<p><img src="/vault/assets/images/flexconv.png" alt="FlexConv parametrization"></p>
<p>In order to sample a kernel, a small, feed-forward subnetwork is used to map relative spatial coordinates sampled at some discrete rate— e.g., a grid on [-1, 1]— to intensities within the desired output space. The output of the MLP can be stabilized with methods such as LayerNorm, weight standardization etc., but the important bit is that the MLP learns a continuous function that maps points in coordinate space to some learned intensity for the output kernel at the given point, rather than fixing the kernel's size and depth and learning the values of each constituent point within the kernel's volumetric mapping individually. </p>
<h2 id="computational-caveats"><a aria-hidden="true" class="anchor-heading icon-link" href="#computational-caveats"></a>Computational Caveats</h2>
<p>Romero, et al. observed based on the states of converged FlexConvs that they tended to learn larger receptive fields as one descends deeper into their architecture during training, at least on natural images. This presented a problem: It slowed things down. The computational overhead of convolutions scales cubically with kernel size, and the only hard upper bound is the input sampling resolution.</p>
<p><img src="/vault/assets/images/progressive-kernel-sizes.png" alt="Progressively Larger Kernel Sizes"></p>
<p>This cost could of course be offset with downsampling, but <a href="https://arxiv.org/abs/2206.03398">CCNN</a> offers evidence that depthwise-separable versions of <a href="https://arxiv.org/abs/2110.08059">FlexConv</a> kernels, or FlexSepConv, can be combined with global kernel sizes or depthwise-separable convolutions in order to offset the computational cost of large kernels, making it quadratic instead. The size of the receptive field produced by kernel generator networks like MAGNets could additionally be conditioned on its inputs with intermediate componentry akin to <a href="/vault/notes/zy4oqk1z59whxqbjtmwbdm4">Feature-wise Linear Modulation</a>, or explicitly regularized with an additional loss term, leading me to ask what the result would be if these architectures were trained with <a href="/vault/notes/0oj3xw4fhwwb7qifcve8g19">Loss Conditioning</a>.</p>
<hr>
<strong>Backlinks</strong>
<ul>
<li><a href="/vault/notes/hqv5s3hlhjqatutciw4x4nm">STEGO (vault)</a></li>
<li><a href="/vault/notes/txmy1anxf7bhr607yyk7te7">DEQ Extensions (vault)</a></li>
</ul></div></div><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-0 ant-col-md-6"><div><div class=""><div class="ant-anchor-wrapper dendron-toc" style="max-height:calc(100vh - 64px);z-index:1"><div class="ant-anchor"><div class="ant-anchor-ink"><span class="ant-anchor-ink-ball"></span></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#computational-caveats" title="Computational Caveats">Computational Caveats</a></div></div></div></div></div></div></div></div></div></main><div class="ant-divider ant-divider-horizontal" role="separator"></div><footer class="ant-layout-footer" style="padding:0 24px 24px"></footer></section></section></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note":{"id":"kxklakzrojw3q2r808mzh65","title":"Continuous Kernel Convolution","desc":"","updated":1659020720817,"created":1658835909947,"custom":{},"fname":"ref.arch.ckconv","type":"note","vault":{"fsPath":"vault"},"contentHash":"b8c09903ec43950ae72cafb19cf17e26","links":[{"type":"wiki","from":{"fname":"ref.arch.ckconv","id":"kxklakzrojw3q2r808mzh65","vaultName":"vault"},"value":"ref.opt.film","alias":"ref.opt.film","position":{"start":{"line":13,"column":491,"offset":2112},"end":{"line":13,"column":507,"offset":2128},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"ref.opt.film"}},{"type":"wiki","from":{"fname":"ref.arch.ckconv","id":"kxklakzrojw3q2r808mzh65","vaultName":"vault"},"value":"ref.opt.loss-conditioning","alias":"ref.opt.loss-conditioning","position":{"start":{"line":13,"column":649,"offset":2270},"end":{"line":13,"column":678,"offset":2299},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"ref.opt.loss-conditioning"}},{"from":{"fname":"ref.cv.stego","vaultName":"vault"},"type":"backlink","position":{"start":{"line":3,"column":211,"offset":570},"end":{"line":3,"column":230,"offset":589},"indent":[]},"value":"ref.arch.ckconv","alias":"ref.arch.ckconv"},{"from":{"fname":"ref.arch.deq.extensions","vaultName":"vault"},"type":"backlink","position":{"start":{"line":14,"column":164,"offset":2431},"end":{"line":14,"column":192,"offset":2459},"indent":[]},"value":"ref.arch.ckconv","alias":"FlexConv"}],"anchors":{"computational-caveats":{"type":"header","text":"Computational Caveats","value":"computational-caveats","line":14,"column":0,"depth":2}},"children":[],"parent":"39acu8nb3xn6t18xul0ji8d","data":{}},"body":"\u003ch1 id=\"continuous-kernel-convolution\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#continuous-kernel-convolution\"\u003e\u003c/a\u003eContinuous Kernel Convolution\u003c/h1\u003e\n\u003cp\u003eContinuous-kernel convolutions are just that: Convolutions with a continuous kernel. The kernel is determined by a \u003cstrong\u003egenerator network\u003c/strong\u003e with a fixed parameter cost, by composing a learned linear operator with a predetermined gaussian mask, such as a blur filter, or an (optionally steerable) Gabor filter as in \u003ca href=\"https://arxiv.org/abs/2110.08059\"\u003eRomero, et al., 2021's\u003c/a\u003e \u003cstrong\u003eM\u003c/strong\u003eultiplicative \u003cstrong\u003eA\u003c/strong\u003enisotropic \u003cstrong\u003eG\u003c/strong\u003eabor \u003cstrong\u003eN\u003c/strong\u003eetworks, or MAGNets.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/vault/assets/images/flexconv.png\" alt=\"FlexConv parametrization\"\u003e\u003c/p\u003e\n\u003cp\u003eIn order to sample a kernel, a small, feed-forward subnetwork is used to map relative spatial coordinates sampled at some discrete rate— e.g., a grid on [-1, 1]— to intensities within the desired output space. The output of the MLP can be stabilized with methods such as LayerNorm, weight standardization etc., but the important bit is that the MLP learns a continuous function that maps points in coordinate space to some learned intensity for the output kernel at the given point, rather than fixing the kernel's size and depth and learning the values of each constituent point within the kernel's volumetric mapping individually. \u003c/p\u003e\n\u003ch2 id=\"computational-caveats\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#computational-caveats\"\u003e\u003c/a\u003eComputational Caveats\u003c/h2\u003e\n\u003cp\u003eRomero, et al. observed based on the states of converged FlexConvs that they tended to learn larger receptive fields as one descends deeper into their architecture during training, at least on natural images. This presented a problem: It slowed things down. The computational overhead of convolutions scales cubically with kernel size, and the only hard upper bound is the input sampling resolution.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/vault/assets/images/progressive-kernel-sizes.png\" alt=\"Progressively Larger Kernel Sizes\"\u003e\u003c/p\u003e\n\u003cp\u003eThis cost could of course be offset with downsampling, but \u003ca href=\"https://arxiv.org/abs/2206.03398\"\u003eCCNN\u003c/a\u003e offers evidence that depthwise-separable versions of \u003ca href=\"https://arxiv.org/abs/2110.08059\"\u003eFlexConv\u003c/a\u003e kernels, or FlexSepConv, can be combined with global kernel sizes or depthwise-separable convolutions in order to offset the computational cost of large kernels, making it quadratic instead. The size of the receptive field produced by kernel generator networks like MAGNets could additionally be conditioned on its inputs with intermediate componentry akin to \u003ca href=\"/vault/notes/zy4oqk1z59whxqbjtmwbdm4\"\u003eFeature-wise Linear Modulation\u003c/a\u003e, or explicitly regularized with an additional loss term, leading me to ask what the result would be if these architectures were trained with \u003ca href=\"/vault/notes/0oj3xw4fhwwb7qifcve8g19\"\u003eLoss Conditioning\u003c/a\u003e.\u003c/p\u003e\n\u003chr\u003e\n\u003cstrong\u003eBacklinks\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/vault/notes/hqv5s3hlhjqatutciw4x4nm\"\u003eSTEGO (vault)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/vault/notes/txmy1anxf7bhr607yyk7te7\"\u003eDEQ Extensions (vault)\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","noteIndex":{"id":"ihfr21mlvz5lp3n62uu344t","title":"kavlogs","desc":"","updated":1659115143090,"created":1659018913792,"custom":{"nav_order":0,"permalink":"/"},"fname":"ref","type":"note","vault":{"fsPath":"vault"},"contentHash":"41b7e8cfe841b035025dfa85ec0b142e","links":[],"anchors":{},"children":["39acu8nb3xn6t18xul0ji8d","jc56h8njgqawadlroex4ju1","wf5gku64lj2ag6n8kzs9o8f","jlbuz8giyj93jsc1l0lmbzs","3c09ct2zvlk9dfitxexbss3"],"parent":null,"data":{},"body":"This is an assorted collection of notes and thoughts on things I've read about before. Here for my reference, and so that you can come here to read my background instead of receiving an untimely info-dump whose content just amounts to a recitation of anything here, out of mutual respect for my time and yours."},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true,"enableSelfContainedVaults":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":true,"vaultSelectionModeOnCreate":"smart","leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2}},"randomNote":{},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"copyNoteLink":{},"templateHierarchy":"template"},"workspace":{"vaults":[{"fsPath":"vault"},{"fsPath":"projects"},{"fsPath":"tenets"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"task":{"name":"task","dateFormat":"y.MM.dd","addBehavior":"asOwnDomain","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"taskCompleteStatus":["done","x"],"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableHandlebarTemplates":true,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"enableUserTags":true,"enableHashTags":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":false,"enableEditorDecorations":true,"maxPreviewsCached":10,"maxNoteLength":204800,"enableFullHierarchyNoteTitle":false,"enableSmartRefs":true,"dendronVersion":"0.105.1"},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":true},"publishing":{"theme":"dark","enableFMTitle":true,"enableNoteTitleForLink":true,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteUrl":"https://kavorite.github.io","assetsPrefix":"/vault","siteHierarchies":["ref","pub"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"⚑","description":"kavlogs"},"github":{"enableEditLink":false,"editBranch":"main","editViewMode":"tree","editLinkText":"Edit this page on GitHub"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"siteFaviconPath":"favicon.ico","siteIndex":"ref"}}},"__N_SSG":true},"page":"/notes/[id]","query":{"id":"kxklakzrojw3q2r808mzh65"},"buildId":"x67zoUTAB0zyGOxjfffun","assetPrefix":"/vault","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>
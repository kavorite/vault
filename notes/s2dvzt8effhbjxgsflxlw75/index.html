<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><link rel="icon" href="/favicon.ico"/><title>UnCLIP</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="kavlogs"/><meta property="og:title" content="UnCLIP"/><meta property="og:description" content="kavlogs"/><meta property="og:url" content="https://kavorite.github.io/vault/notes/s2dvzt8effhbjxgsflxlw75/"/><meta property="og:type" content="article"/><meta property="article:published_time" content="7/27/2022"/><meta property="article:modified_time" content="8/2/2022"/><link rel="canonical" href="https://kavorite.github.io/vault/notes/s2dvzt8effhbjxgsflxlw75/"/><meta name="next-head-count" content="14"/><link rel="preload" href="/vault/_next/static/css/bec73badd4e5ba88.css" as="style"/><link rel="stylesheet" href="/vault/_next/static/css/bec73badd4e5ba88.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/vault/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/vault/_next/static/chunks/webpack-3af8c0cde089b23c.js" defer=""></script><script src="/vault/_next/static/chunks/framework-bb5c596eafb42b22.js" defer=""></script><script src="/vault/_next/static/chunks/main-34d019fcedb697b6.js" defer=""></script><script src="/vault/_next/static/chunks/pages/_app-b20c4be9bb651ea8.js" defer=""></script><script src="/vault/_next/static/chunks/826-e0e455fb469c158f.js" defer=""></script><script src="/vault/_next/static/chunks/986-737e5da213076068.js" defer=""></script><script src="/vault/_next/static/chunks/pages/notes/%5Bid%5D-00dd1421f3ce3a3e.js" defer=""></script><script src="/vault/_next/static/x67zoUTAB0zyGOxjfffun/_buildManifest.js" defer=""></script><script src="/vault/_next/static/x67zoUTAB0zyGOxjfffun/_ssgManifest.js" defer=""></script><script src="/vault/_next/static/x67zoUTAB0zyGOxjfffun/_middlewareManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><section class="ant-layout" style="width:100%;min-height:100%"><header class="ant-layout-header" style="position:fixed;isolation:isolate;z-index:1;width:100%;border-bottom:1px solid #d4dadf;height:64px;padding:0 24px 0 2px"><div class="ant-row ant-row-center" style="max-width:992px;justify-content:space-between;margin:0 auto"><div style="display:flex" class="ant-col ant-col-xs-20 ant-col-sm-4"></div><div class="ant-col gutter-row ant-col-xs-0 ant-col-sm-20 ant-col-md-20 ant-col-lg-19"><div class="ant-select ant-select-lg ant-select-auto-complete ant-select-single ant-select-allow-clear ant-select-show-search" style="width:100%"><div class="ant-select-selector"><span class="ant-select-selection-search"><input type="search" autoComplete="off" class="ant-select-selection-search-input" role="combobox" aria-haspopup="listbox" aria-owns="undefined_list" aria-autocomplete="list" aria-controls="undefined_list" aria-activedescendant="undefined_list_0" value=""/></span><span class="ant-select-selection-placeholder">For full text search please use the &#x27;?&#x27; prefix. e.g. ? Onboarding</span></div></div></div><div style="display:none;align-items:center;justify-content:center" class="ant-col ant-col-xs-4 ant-col-sm-4 ant-col-md-0 ant-col-lg-0"><span role="img" aria-label="menu" style="font-size:24px" tabindex="-1" class="anticon anticon-menu"><svg viewBox="64 64 896 896" focusable="false" data-icon="menu" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M904 160H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0 624H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0-312H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8z"></path></svg></span></div></div></header><section class="ant-layout" style="margin-top:64px;display:flex;flex-direction:row"><div class="site-layout-sidebar" style="flex:0 0 auto;width:calc(max((100% - 992px) / 2, 0px) + 200px);min-width:200px;padding-left:calc((100% - 992px) / 2)"><aside class="ant-layout-sider ant-layout-sider-dark" style="position:fixed;overflow:auto;height:calc(100vh - 64px);background-color:transparent;flex:0 0 200px;max-width:200px;min-width:200px;width:200px"><div class="ant-layout-sider-children"></div></aside></div><section class="ant-layout side-layout-main" style="max-width:1200px;display:initial"><main class="ant-layout-content main-content" role="main" style="padding:0 24px"><div class="ant-row"><div class="ant-col ant-col-24"><div class="ant-row" style="margin-left:-10px;margin-right:-10px"><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-24 ant-col-md-18"><div><h1 id="unclip"><a aria-hidden="true" class="anchor-heading icon-link" href="#unclip"></a>UnCLIP</h1>
<p><a href="https://cdn.openai.com/papers/dall-e-2.pdf">Dall⋅E</a> performs image generation and editing conditioned on semantic natural language prompts embedded in the prior given by a multimodal <a href="/vault/notes/63z6zjmnp5tg1pj2rwcfb2f">Contrastive Language-Image Pretraining</a> encoder. Most of the publication just talks about jointly training a transformer textual encoder with an image encoder. As seems to be OpenAI's trademark, most of this discussion is making a lot of hay over very little actual novelty in its methods, and is mostly just impressive in its thorough enumeration of implementation details and sheer scale. </p>
<p>The real meat of the publication lies in the review of a number of different approaches from literature for methods of image synthesis (<a href="https://cdn.openai.com/papers/dall-e-2.pdf">paper</a> Section 2, "Method"), which reconstruct images by conditioning on both explicit textual prompts and latent multimodal embeddings. This process is not trained jointly with CLIP. Instead, the "text-image codebook" is learned ahead of time. <a href="https://openreview.net/pdf?id=qw8AKxfYbI">Classifier-free guidance</a> is implemented by randomly dropping CLIP embeddings during UnCLIP's training regime.</p>
<p>The best confluence of sample diversity, visual fidelity, and aesthetic ratings by human viewers was found to use an ensemble of <a href="https://arxiv.org/abs/2010.02502">DDIM</a> models which feed forward into one another with intermediate steps which upsample to a higher resolution (64×64, 256×256, 1024×1024), although it is unclear from the paper exactly how each stage is parametrized. This three-stage pipeline and the intermediate upsampling steps are collectively called UnCLIP. In this regime, each successive upsampled image is noised essentially by applying a transposed block filter, rather than by directly sampling from a Gaussian distribution. </p>
<p>Denoising the image via inpainting while reproducing the same semantics as the original content becomes a nontrivial, denoising-autoencoding objective for which diffusion models were found to be best suited overall. Perhaps a decoder could perform better by allowing the upsampling factor to be conditioned on the input using <a href="/vault/notes/zy4oqk1z59whxqbjtmwbdm4">Feature-wise Linear Modulation</a> and dynamically learned?</p>
<hr>
<strong>Backlinks</strong>
<ul>
<li><a href="/vault/notes/txmy1anxf7bhr607yyk7te7">DEQ Extensions (vault)</a></li>
</ul></div></div><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-0 ant-col-md-6"><div><div class=""><div class="ant-anchor-wrapper dendron-toc" style="max-height:calc(100vh - 64px);z-index:1"><div class="ant-anchor"><div class="ant-anchor-ink"><span class="ant-anchor-ink-ball"></span></div></div></div></div></div></div></div></div></div></main><div class="ant-divider ant-divider-horizontal" role="separator"></div><footer class="ant-layout-footer" style="padding:0 24px 24px"></footer></section></section></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note":{"id":"s2dvzt8effhbjxgsflxlw75","title":"UnCLIP","desc":"","updated":1659409460728,"created":1658923989156,"custom":{},"fname":"ref.cv.unclip","type":"note","vault":{"fsPath":"vault"},"contentHash":"2974e285cdeb3869a1efec1f3cb5048b","links":[{"type":"wiki","from":{"fname":"ref.cv.unclip","id":"s2dvzt8effhbjxgsflxlw75","vaultName":"vault"},"value":"ref.cl.clip","alias":"ref.cl.clip","position":{"start":{"line":2,"column":148,"offset":148},"end":{"line":2,"column":163,"offset":163},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"ref.cl.clip"}},{"type":"wiki","from":{"fname":"ref.cv.unclip","id":"s2dvzt8effhbjxgsflxlw75","vaultName":"vault"},"value":"ref.opt.film","alias":"ref.opt.film","position":{"start":{"line":8,"column":327,"offset":1964},"end":{"line":8,"column":343,"offset":1980},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"ref.opt.film"}},{"from":{"fname":"ref.arch.deq.extensions","vaultName":"vault"},"type":"backlink","position":{"start":{"line":12,"column":786,"offset":2223},"end":{"line":12,"column":803,"offset":2240},"indent":[]},"value":"ref.cv.unclip","alias":"ref.cv.unclip"}],"anchors":{},"children":[],"parent":"wf5gku64lj2ag6n8kzs9o8f","data":{}},"body":"\u003ch1 id=\"unclip\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#unclip\"\u003e\u003c/a\u003eUnCLIP\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://cdn.openai.com/papers/dall-e-2.pdf\"\u003eDall⋅E\u003c/a\u003e performs image generation and editing conditioned on semantic natural language prompts embedded in the prior given by a multimodal \u003ca href=\"/vault/notes/63z6zjmnp5tg1pj2rwcfb2f\"\u003eContrastive Language-Image Pretraining\u003c/a\u003e encoder. Most of the publication just talks about jointly training a transformer textual encoder with an image encoder. As seems to be OpenAI's trademark, most of this discussion is making a lot of hay over very little actual novelty in its methods, and is mostly just impressive in its thorough enumeration of implementation details and sheer scale. \u003c/p\u003e\n\u003cp\u003eThe real meat of the publication lies in the review of a number of different approaches from literature for methods of image synthesis (\u003ca href=\"https://cdn.openai.com/papers/dall-e-2.pdf\"\u003epaper\u003c/a\u003e Section 2, \"Method\"), which reconstruct images by conditioning on both explicit textual prompts and latent multimodal embeddings. This process is not trained jointly with CLIP. Instead, the \"text-image codebook\" is learned ahead of time. \u003ca href=\"https://openreview.net/pdf?id=qw8AKxfYbI\"\u003eClassifier-free guidance\u003c/a\u003e is implemented by randomly dropping CLIP embeddings during UnCLIP's training regime.\u003c/p\u003e\n\u003cp\u003eThe best confluence of sample diversity, visual fidelity, and aesthetic ratings by human viewers was found to use an ensemble of \u003ca href=\"https://arxiv.org/abs/2010.02502\"\u003eDDIM\u003c/a\u003e models which feed forward into one another with intermediate steps which upsample to a higher resolution (64×64, 256×256, 1024×1024), although it is unclear from the paper exactly how each stage is parametrized. This three-stage pipeline and the intermediate upsampling steps are collectively called UnCLIP. In this regime, each successive upsampled image is noised essentially by applying a transposed block filter, rather than by directly sampling from a Gaussian distribution. \u003c/p\u003e\n\u003cp\u003eDenoising the image via inpainting while reproducing the same semantics as the original content becomes a nontrivial, denoising-autoencoding objective for which diffusion models were found to be best suited overall. Perhaps a decoder could perform better by allowing the upsampling factor to be conditioned on the input using \u003ca href=\"/vault/notes/zy4oqk1z59whxqbjtmwbdm4\"\u003eFeature-wise Linear Modulation\u003c/a\u003e and dynamically learned?\u003c/p\u003e\n\u003chr\u003e\n\u003cstrong\u003eBacklinks\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/vault/notes/txmy1anxf7bhr607yyk7te7\"\u003eDEQ Extensions (vault)\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","noteIndex":{"id":"ihfr21mlvz5lp3n62uu344t","title":"kavlogs","desc":"","updated":1659115143090,"created":1659018913792,"custom":{"nav_order":0,"permalink":"/"},"fname":"ref","type":"note","vault":{"fsPath":"vault"},"contentHash":"41b7e8cfe841b035025dfa85ec0b142e","links":[],"anchors":{},"children":["39acu8nb3xn6t18xul0ji8d","jc56h8njgqawadlroex4ju1","wf5gku64lj2ag6n8kzs9o8f","jlbuz8giyj93jsc1l0lmbzs","3c09ct2zvlk9dfitxexbss3"],"parent":null,"data":{},"body":"This is an assorted collection of notes and thoughts on things I've read about before. Here for my reference, and so that you can come here to read my background instead of receiving an untimely info-dump whose content just amounts to a recitation of anything here, out of mutual respect for my time and yours."},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true,"enableSelfContainedVaults":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":true,"vaultSelectionModeOnCreate":"smart","leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2}},"randomNote":{},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"copyNoteLink":{},"templateHierarchy":"template"},"workspace":{"vaults":[{"fsPath":"vault"},{"fsPath":"projects"},{"fsPath":"tenets"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"task":{"name":"task","dateFormat":"y.MM.dd","addBehavior":"asOwnDomain","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"taskCompleteStatus":["done","x"],"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableHandlebarTemplates":true,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"enableUserTags":true,"enableHashTags":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":false,"enableEditorDecorations":true,"maxPreviewsCached":10,"maxNoteLength":204800,"enableFullHierarchyNoteTitle":false,"enableSmartRefs":true,"dendronVersion":"0.105.1"},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":true},"publishing":{"theme":"dark","enableFMTitle":true,"enableNoteTitleForLink":true,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteUrl":"https://kavorite.github.io","assetsPrefix":"/vault","siteHierarchies":["ref","pub"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"⚑","description":"kavlogs"},"github":{"enableEditLink":false,"editBranch":"main","editViewMode":"tree","editLinkText":"Edit this page on GitHub"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"siteFaviconPath":"favicon.ico","siteIndex":"ref"}}},"__N_SSG":true},"page":"/notes/[id]","query":{"id":"s2dvzt8effhbjxgsflxlw75"},"buildId":"x67zoUTAB0zyGOxjfffun","assetPrefix":"/vault","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>